WARNING:tensorflow:From /home/projects/ku_00062/envs/iglue/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/projects/ku_00062/envs/iglue/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/projects/ku_00062/envs/iglue/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.

12/03/2021 03:06:27 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False
12/03/2021 03:06:27 - INFO - volta.task_utils -   Loading GQA Dataset with batch size 64
12/03/2021 03:06:27 - INFO - volta.train_utils -   logging file at: /home/projects/ku_00062/logs/iglue/xgqa/ctrl_lxmert_base/GQA_ctrl_lxmert_base
12/03/2021 03:06:27 - INFO - volta.utils -   loading weights file /home/projects/ku_00062/checkpoints/iglue/pretrain/ctrl_lxmert/ctrl_lxmert_base/pytorch_model_9.bin
12/03/2021 03:06:34 - INFO - volta.utils -   
12/03/2021 03:06:34 - INFO - volta.utils -   Weights of BertForVLTasks not initialized from pretrained model: ['clfs_dict.TASK15.logit_fc.0.weight', 'clfs_dict.TASK15.logit_fc.0.bias', 'clfs_dict.TASK15.logit_fc.2.weight', 'clfs_dict.TASK15.logit_fc.2.bias', 'clfs_dict.TASK15.logit_fc.3.weight', 'clfs_dict.TASK15.logit_fc.3.bias']
12/03/2021 03:06:34 - INFO - volta.utils -   Weights from pretrained model not used in BertForVLTasks: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.bi_seq_relationship.weight', 'cls.bi_seq_relationship.bias', 'cls.imagePredictions.transform.dense.weight', 'cls.imagePredictions.transform.dense.bias', 'cls.imagePredictions.transform.LayerNorm.weight', 'cls.imagePredictions.transform.LayerNorm.bias', 'cls.imagePredictions.decoder_dict.0.weight', 'cls.imagePredictions.decoder_dict.0.bias']
12/03/2021 03:06:37 - INFO - __main__ -   ** ** * Saving model * ** ** 
/home/projects/ku_00062/envs/iglue/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.
  warnings.warn(SAVE_STATE_WARNING, UserWarning)
12/03/2021 03:06:44 - INFO - __main__ -   >> Parameters:
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |Name                                                         |Dtype            |Shape           |#Params     |Trainable|
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.embeddings.word_embeddings.weight                       |torch.float32    |(30522, 768)    |23440896    |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.embeddings.position_embeddings.weight                   |torch.float32    |(512, 768)      |393216      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.embeddings.token_type_embeddings.weight                 |torch.float32    |(2, 768)        |1536        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.embeddings.LayerNorm.weight                             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.embeddings.LayerNorm.bias                               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.image_embeddings.weight                    |torch.float32    |(768, 2048)     |1572864     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.image_embeddings.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.image_location_embeddings.weight           |torch.float32    |(768, 5)        |3840        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.image_location_embeddings.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.ImgLayerNorm.weight                        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.ImgLayerNorm.bias                          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.LocLayerNorm.weight                        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_embeddings.LocLayerNorm.bias                          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.query.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.key.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.value.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_query.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_query.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_key.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_key.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_value.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_self.v_value.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.dense.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.v_dense.weight         |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.v_dense.bias           |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.v_LayerNorm.weight     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.0.attention_output.v_LayerNorm.bias       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.intermediate.v_dense.weight             |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.intermediate.v_dense.bias               |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.dense.bias                       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.v_dense.weight                   |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.v_dense.bias                     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.v_LayerNorm.weight               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.1.output.v_LayerNorm.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.query.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.key.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.value.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_query.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_query.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_key.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_key.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_value.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_self.v_value.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.dense.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.v_dense.weight         |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.v_dense.bias           |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.v_LayerNorm.weight     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.2.attention_output.v_LayerNorm.bias       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.intermediate.v_dense.weight             |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.intermediate.v_dense.bias               |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.dense.bias                       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.v_dense.weight                   |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.v_dense.bias                     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.v_LayerNorm.weight               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.3.output.v_LayerNorm.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.query.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.key.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.value.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_query.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_query.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_key.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_key.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_value.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_self.v_value.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.dense.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.v_dense.weight         |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.v_dense.bias           |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.v_LayerNorm.weight     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.4.attention_output.v_LayerNorm.bias       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.intermediate.v_dense.weight             |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.intermediate.v_dense.bias               |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.dense.bias                       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.v_dense.weight                   |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.v_dense.bias                     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.v_LayerNorm.weight               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.5.output.v_LayerNorm.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.query.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.key.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.value.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_query.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_query.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_key.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_key.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_value.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_self.v_value.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.dense.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.v_dense.weight         |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.v_dense.bias           |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.v_LayerNorm.weight     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.6.attention_output.v_LayerNorm.bias       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.intermediate.v_dense.weight             |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.intermediate.v_dense.bias               |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.dense.bias                       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.v_dense.weight                   |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.v_dense.bias                     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.v_LayerNorm.weight               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.7.output.v_LayerNorm.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.query.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.query.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.key.weight               |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.key.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.value.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.value.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_query.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_query.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_key.weight             |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_key.bias               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_value.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_self.v_value.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.dense.weight           |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.dense.bias             |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.LayerNorm.weight       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.LayerNorm.bias         |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.v_dense.weight         |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.v_dense.bias           |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.v_LayerNorm.weight     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.8.attention_output.v_LayerNorm.bias       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.intermediate.dense.weight               |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.intermediate.dense.bias                 |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.intermediate.v_dense.weight             |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.intermediate.v_dense.bias               |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.dense.weight                     |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.dense.bias                       |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.LayerNorm.weight                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.LayerNorm.bias                   |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.v_dense.weight                   |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.v_dense.bias                     |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.v_LayerNorm.weight               |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.9.output.v_LayerNorm.bias                 |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.10.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.11.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.12.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.13.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.14.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.15.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.16.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.17.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.18.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_query.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_query.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_key.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_key.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_value.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_self.v_value.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.v_dense.weight        |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.v_dense.bias          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.v_LayerNorm.weight    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.19.attention_output.v_LayerNorm.bias      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.intermediate.v_dense.weight            |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.intermediate.v_dense.bias              |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.v_dense.weight                  |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.v_dense.bias                    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.v_LayerNorm.weight              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.20.output.v_LayerNorm.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.21.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_query.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_query.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_key.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_key.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_value.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_self.v_value.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.v_dense.weight        |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.v_dense.bias          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.v_LayerNorm.weight    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.22.attention_output.v_LayerNorm.bias      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.intermediate.v_dense.weight            |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.intermediate.v_dense.bias              |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.v_dense.weight                  |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.v_dense.bias                    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.v_LayerNorm.weight              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.23.output.v_LayerNorm.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.24.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_query.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_query.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_key.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_key.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_value.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_self.v_value.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.v_dense.weight        |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.v_dense.bias          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.v_LayerNorm.weight    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.25.attention_output.v_LayerNorm.bias      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.intermediate.v_dense.weight            |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.intermediate.v_dense.bias              |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.v_dense.weight                  |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.v_dense.bias                    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.v_LayerNorm.weight              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.26.output.v_LayerNorm.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.27.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_query.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_query.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_key.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_key.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_value.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_self.v_value.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.v_dense.weight        |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.v_dense.bias          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.v_LayerNorm.weight    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.28.attention_output.v_LayerNorm.bias      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.intermediate.v_dense.weight            |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.intermediate.v_dense.bias              |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.v_dense.weight                  |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.v_dense.bias                    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.v_LayerNorm.weight              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.29.output.v_LayerNorm.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.30.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.query.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.query.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.key.weight              |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.key.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.value.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.value.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_query.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_query.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_key.weight            |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_key.bias              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_value.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_self.v_value.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.dense.weight          |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.dense.bias            |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.LayerNorm.weight      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.LayerNorm.bias        |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.v_dense.weight        |torch.float32    |(768, 768)      |589824      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.v_dense.bias          |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.v_LayerNorm.weight    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.31.attention_output.v_LayerNorm.bias      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.intermediate.dense.weight              |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.intermediate.dense.bias                |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.intermediate.v_dense.weight            |torch.float32    |(3072, 768)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.intermediate.v_dense.bias              |torch.float32    |(3072,)         |3072        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.dense.weight                    |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.dense.bias                      |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.LayerNorm.weight                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.LayerNorm.bias                  |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.v_dense.weight                  |torch.float32    |(768, 3072)     |2359296     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.v_dense.bias                    |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.v_LayerNorm.weight              |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.encoder.layer.32.output.v_LayerNorm.bias                |torch.float32    |(768,)          |768         |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.t_pooler.dense.weight                                   |torch.float32    |(1024, 768)     |786432      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.t_pooler.dense.bias                                     |torch.float32    |(1024,)         |1024        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_pooler.dense.weight                                   |torch.float32    |(1024, 768)     |786432      |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |bert.v_pooler.dense.bias                                     |torch.float32    |(1024,)         |1024        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.0.weight                           |torch.float32    |(1536, 1024)    |1572864     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.0.bias                             |torch.float32    |(1536,)         |1536        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.2.weight                           |torch.float32    |(1536,)         |1536        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.2.bias                             |torch.float32    |(1536,)         |1536        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.3.weight                           |torch.float32    |(1842, 1536)    |2829312     |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   |clfs_dict.TASK15.logit_fc.3.bias                             |torch.float32    |(1842,)         |1842        |True    |
12/03/2021 03:06:44 - INFO - __main__ -   ------------------------------------------------------------------------------------------------------------------------
12/03/2021 03:06:44 - INFO - __main__ -   >> # TrainableParams:       	213.33	M
12/03/2021 03:06:44 - INFO - __main__ -   >> # NonTrainableParams:    	0.00	M
12/03/2021 03:06:44 - INFO - __main__ -   >> # TotalParams:           	213.33	M
Epoch:   0%|          | 0/5 [00:00<?, ?it/s]/home/projects/ku_00062/envs/iglue/lib/python3.6/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
12/03/2021 03:07:16 - INFO - volta.train_utils -   [GQA]: iter 80 Ep: 0.01 loss 87.468 score 0.000 lr 1.42527e-08 
12/03/2021 03:07:48 - INFO - volta.train_utils -   [GQA]: iter 160 Ep: 0.01 loss 87.444 score 0.000 lr 4.14008e-08 
12/03/2021 03:08:20 - INFO - volta.train_utils -   [GQA]: iter 240 Ep: 0.02 loss 87.076 score 0.000 lr 6.85489e-08 
12/03/2021 03:08:52 - INFO - volta.train_utils -   [GQA]: iter 320 Ep: 0.02 loss 86.628 score 0.000 lr 9.5697e-08 
12/03/2021 03:09:24 - INFO - volta.train_utils -   [GQA]: iter 400 Ep: 0.03 loss 86.119 score 0.000 lr 1.22845e-07 
12/03/2021 03:09:57 - INFO - volta.train_utils -   [GQA]: iter 480 Ep: 0.03 loss 85.404 score 0.000 lr 1.49993e-07 
12/03/2021 03:10:29 - INFO - volta.train_utils -   [GQA]: iter 560 Ep: 0.04 loss 84.370 score 0.000 lr 1.77141e-07 
12/03/2021 03:11:01 - INFO - volta.train_utils -   [GQA]: iter 640 Ep: 0.04 loss 83.067 score 0.000 lr 2.04289e-07 
12/03/2021 03:11:34 - INFO - volta.train_utils -   [GQA]: iter 720 Ep: 0.05 loss 81.340 score 0.000 lr 2.31437e-07 
12/03/2021 03:12:06 - INFO - volta.train_utils -   [GQA]: iter 800 Ep: 0.05 loss 79.024 score 0.000 lr 2.58586e-07 
12/03/2021 03:12:39 - INFO - volta.train_utils -   [GQA]: iter 880 Ep: 0.06 loss 76.389 score 0.000 lr 2.85734e-07 
12/03/2021 03:13:11 - INFO - volta.train_utils -   [GQA]: iter 960 Ep: 0.07 loss 73.419 score 0.000 lr 3.12882e-07 
12/03/2021 03:13:43 - INFO - volta.train_utils -   [GQA]: iter 1040 Ep: 0.07 loss 70.115 score 0.000 lr 3.4003e-07 
12/03/2021 03:14:15 - INFO - volta.train_utils -   [GQA]: iter 1120 Ep: 0.08 loss 66.637 score 0.000 lr 3.67178e-07 
12/03/2021 03:14:47 - INFO - volta.train_utils -   [GQA]: iter 1200 Ep: 0.08 loss 63.134 score 0.000 lr 3.94326e-07 
12/03/2021 03:15:20 - INFO - volta.train_utils -   [GQA]: iter 1280 Ep: 0.09 loss 59.628 score 0.000 lr 4.21474e-07 
12/03/2021 03:15:55 - INFO - volta.train_utils -   [GQA]: iter 1360 Ep: 0.09 loss 56.230 score 0.000 lr 4.48622e-07 
12/03/2021 03:16:30 - INFO - volta.train_utils -   [GQA]: iter 1440 Ep: 0.10 loss 52.998 score 0.000 lr 4.7577e-07 
12/03/2021 03:17:02 - INFO - volta.train_utils -   [GQA]: iter 1520 Ep: 0.10 loss 49.748 score 0.000 lr 5.02918e-07 
12/03/2021 03:17:35 - INFO - volta.train_utils -   [GQA]: iter 1600 Ep: 0.11 loss 46.631 score 0.000 lr 5.30067e-07 
12/03/2021 03:18:07 - INFO - volta.train_utils -   [GQA]: iter 1680 Ep: 0.11 loss 43.615 score 0.000 lr 5.57215e-07 
12/03/2021 03:18:40 - INFO - volta.train_utils -   [GQA]: iter 1760 Ep: 0.12 loss 40.668 score 0.000 lr 5.84363e-07 
12/03/2021 03:19:13 - INFO - volta.train_utils -   [GQA]: iter 1840 Ep: 0.12 loss 37.732 score 0.000 lr 6.11511e-07 
12/03/2021 03:19:45 - INFO - volta.train_utils -   [GQA]: iter 1920 Ep: 0.13 loss 34.871 score 0.000 lr 6.38659e-07 
12/03/2021 03:20:18 - INFO - volta.train_utils -   [GQA]: iter 2000 Ep: 0.14 loss 32.143 score 0.000 lr 6.65807e-07 
12/03/2021 03:20:50 - INFO - volta.train_utils -   [GQA]: iter 2080 Ep: 0.14 loss 29.438 score 0.000 lr 6.92955e-07 
12/03/2021 03:21:23 - INFO - volta.train_utils -   [GQA]: iter 2160 Ep: 0.15 loss 26.844 score 0.000 lr 7.20103e-07 
12/03/2021 03:21:56 - INFO - volta.train_utils -   [GQA]: iter 2240 Ep: 0.15 loss 24.387 score 0.000 lr 7.47251e-07 
12/03/2021 03:22:28 - INFO - volta.train_utils -   [GQA]: iter 2320 Ep: 0.16 loss 22.015 score 0.000 lr 7.74399e-07 
12/03/2021 03:23:01 - INFO - volta.train_utils -   [GQA]: iter 2400 Ep: 0.16 loss 19.802 score 0.000 lr 8.01547e-07 
12/03/2021 03:23:37 - INFO - volta.train_utils -   [GQA]: iter 2480 Ep: 0.17 loss 17.731 score 0.000 lr 8.28696e-07 
12/03/2021 03:24:10 - INFO - volta.train_utils -   [GQA]: iter 2560 Ep: 0.17 loss 15.814 score 0.000 lr 8.55844e-07 
12/03/2021 03:24:42 - INFO - volta.train_utils -   [GQA]: iter 2640 Ep: 0.18 loss 14.014 score 0.000 lr 8.82992e-07 
12/03/2021 03:25:15 - INFO - volta.train_utils -   [GQA]: iter 2720 Ep: 0.18 loss 12.416 score 0.000 lr 9.1014e-07 
12/03/2021 03:25:50 - INFO - volta.train_utils -   [GQA]: iter 2800 Ep: 0.19 loss 10.913 score 0.000 lr 9.37288e-07 
12/03/2021 03:26:23 - INFO - volta.train_utils -   [GQA]: iter 2880 Ep: 0.20 loss 9.584 score 0.001 lr 9.64436e-07 
12/03/2021 03:26:55 - INFO - volta.train_utils -   [GQA]: iter 2960 Ep: 0.20 loss 8.357 score 0.003 lr 9.91584e-07 
12/03/2021 03:27:28 - INFO - volta.train_utils -   [GQA]: iter 3040 Ep: 0.21 loss 7.284 score 0.013 lr 1.01873e-06 
12/03/2021 03:28:01 - INFO - volta.train_utils -   [GQA]: iter 3120 Ep: 0.21 loss 6.317 score 0.029 lr 1.04588e-06 
12/03/2021 03:28:33 - INFO - volta.train_utils -   [GQA]: iter 3200 Ep: 0.22 loss 5.482 score 0.036 lr 1.07303e-06 
12/03/2021 03:29:06 - INFO - volta.train_utils -   [GQA]: iter 3280 Ep: 0.22 loss 4.725 score 0.044 lr 1.10018e-06 
12/03/2021 03:29:38 - INFO - volta.train_utils -   [GQA]: iter 3360 Ep: 0.23 loss 4.073 score 0.046 lr 1.12732e-06 
12/03/2021 03:30:11 - INFO - volta.train_utils -   [GQA]: iter 3440 Ep: 0.23 loss 3.495 score 0.042 lr 1.15447e-06 
12/03/2021 03:30:44 - INFO - volta.train_utils -   [GQA]: iter 3520 Ep: 0.24 loss 2.998 score 0.044 lr 1.18162e-06 
12/03/2021 03:31:17 - INFO - volta.train_utils -   [GQA]: iter 3600 Ep: 0.24 loss 2.576 score 0.044 lr 1.20877e-06 
12/03/2021 03:31:52 - INFO - volta.train_utils -   [GQA]: iter 3680 Ep: 0.25 loss 2.214 score 0.044 lr 1.23592e-06 
12/03/2021 03:32:24 - INFO - volta.train_utils -   [GQA]: iter 3760 Ep: 0.26 loss 1.896 score 0.044 lr 1.26307e-06 
12/03/2021 03:32:57 - INFO - volta.train_utils -   [GQA]: iter 3840 Ep: 0.26 loss 1.630 score 0.042 lr 1.29021e-06 
12/03/2021 03:33:30 - INFO - volta.train_utils -   [GQA]: iter 3920 Ep: 0.27 loss 1.405 score 0.042 lr 1.31736e-06 
12/03/2021 03:34:03 - INFO - volta.train_utils -   [GQA]: iter 4000 Ep: 0.27 loss 1.212 score 0.045 lr 1.34451e-06 
12/03/2021 03:34:36 - INFO - volta.train_utils -   [GQA]: iter 4080 Ep: 0.28 loss 1.058 score 0.044 lr 1.37166e-06 
12/03/2021 03:35:10 - INFO - volta.train_utils -   [GQA]: iter 4160 Ep: 0.28 loss 0.923 score 0.044 lr 1.39881e-06 
12/03/2021 03:35:43 - INFO - volta.train_utils -   [GQA]: iter 4240 Ep: 0.29 loss 0.817 score 0.046 lr 1.42595e-06 
12/03/2021 03:36:15 - INFO - volta.train_utils -   [GQA]: iter 4320 Ep: 0.29 loss 0.719 score 0.046 lr 1.4531e-06 
12/03/2021 03:36:48 - INFO - volta.train_utils -   [GQA]: iter 4400 Ep: 0.30 loss 0.651 score 0.043 lr 1.48025e-06 
12/03/2021 03:37:21 - INFO - volta.train_utils -   [GQA]: iter 4480 Ep: 0.30 loss 0.579 score 0.047 lr 1.5074e-06 
12/03/2021 03:37:53 - INFO - volta.train_utils -   [GQA]: iter 4560 Ep: 0.31 loss 0.537 score 0.046 lr 1.53455e-06 
12/03/2021 03:38:26 - INFO - volta.train_utils -   [GQA]: iter 4640 Ep: 0.31 loss 0.497 score 0.054 lr 1.56169e-06 
12/03/2021 03:38:59 - INFO - volta.train_utils -   [GQA]: iter 4720 Ep: 0.32 loss 0.456 score 0.056 lr 1.58884e-06 
12/03/2021 03:39:33 - INFO - volta.train_utils -   [GQA]: iter 4800 Ep: 0.33 loss 0.436 score 0.056 lr 1.61599e-06 
12/03/2021 03:40:06 - INFO - volta.train_utils -   [GQA]: iter 4880 Ep: 0.33 loss 0.419 score 0.054 lr 1.64314e-06 
12/03/2021 03:40:39 - INFO - volta.train_utils -   [GQA]: iter 4960 Ep: 0.34 loss 0.395 score 0.057 lr 1.67029e-06 
12/03/2021 03:41:12 - INFO - volta.train_utils -   [GQA]: iter 5040 Ep: 0.34 loss 0.380 score 0.057 lr 1.69743e-06 
12/03/2021 03:41:44 - INFO - volta.train_utils -   [GQA]: iter 5120 Ep: 0.35 loss 0.365 score 0.058 lr 1.72458e-06 
12/03/2021 03:42:17 - INFO - volta.train_utils -   [GQA]: iter 5200 Ep: 0.35 loss 0.358 score 0.061 lr 1.75173e-06 
12/03/2021 03:42:49 - INFO - volta.train_utils -   [GQA]: iter 5280 Ep: 0.36 loss 0.351 score 0.064 lr 1.77888e-06 
12/03/2021 03:43:22 - INFO - volta.train_utils -   [GQA]: iter 5360 Ep: 0.36 loss 0.338 score 0.064 lr 1.80603e-06 
12/03/2021 03:43:56 - INFO - volta.train_utils -   [GQA]: iter 5440 Ep: 0.37 loss 0.332 score 0.067 lr 1.83317e-06 
12/03/2021 03:44:29 - INFO - volta.train_utils -   [GQA]: iter 5520 Ep: 0.37 loss 0.319 score 0.066 lr 1.86032e-06 
12/03/2021 03:45:02 - INFO - volta.train_utils -   [GQA]: iter 5600 Ep: 0.38 loss 0.325 score 0.065 lr 1.88747e-06 
12/03/2021 03:45:34 - INFO - volta.train_utils -   [GQA]: iter 5680 Ep: 0.39 loss 0.320 score 0.068 lr 1.91462e-06 
12/03/2021 03:46:07 - INFO - volta.train_utils -   [GQA]: iter 5760 Ep: 0.39 loss 0.302 score 0.069 lr 1.94177e-06 
12/03/2021 03:46:40 - INFO - volta.train_utils -   [GQA]: iter 5840 Ep: 0.40 loss 0.287 score 0.075 lr 1.96892e-06 
12/03/2021 03:47:13 - INFO - volta.train_utils -   [GQA]: iter 5920 Ep: 0.40 loss 0.285 score 0.072 lr 1.99606e-06 
12/03/2021 03:47:46 - INFO - volta.train_utils -   [GQA]: iter 6000 Ep: 0.41 loss 0.295 score 0.074 lr 2.02321e-06 
12/03/2021 03:48:19 - INFO - volta.train_utils -   [GQA]: iter 6080 Ep: 0.41 loss 0.287 score 0.070 lr 2.05036e-06 
12/03/2021 03:48:52 - INFO - volta.train_utils -   [GQA]: iter 6160 Ep: 0.42 loss 0.281 score 0.076 lr 2.07751e-06 
12/03/2021 03:49:24 - INFO - volta.train_utils -   [GQA]: iter 6240 Ep: 0.42 loss 0.277 score 0.070 lr 2.10466e-06 
12/03/2021 03:49:57 - INFO - volta.train_utils -   [GQA]: iter 6320 Ep: 0.43 loss 0.268 score 0.075 lr 2.1318e-06 
12/03/2021 03:50:29 - INFO - volta.train_utils -   [GQA]: iter 6400 Ep: 0.43 loss 0.267 score 0.075 lr 2.15895e-06 
12/03/2021 03:51:02 - INFO - volta.train_utils -   [GQA]: iter 6480 Ep: 0.44 loss 0.257 score 0.082 lr 2.1861e-06 
12/03/2021 03:51:35 - INFO - volta.train_utils -   [GQA]: iter 6560 Ep: 0.45 loss 0.266 score 0.078 lr 2.21325e-06 
12/03/2021 03:52:08 - INFO - volta.train_utils -   [GQA]: iter 6640 Ep: 0.45 loss 0.266 score 0.077 lr 2.2404e-06 
12/03/2021 03:52:40 - INFO - volta.train_utils -   [GQA]: iter 6720 Ep: 0.46 loss 0.263 score 0.079 lr 2.26754e-06 
12/03/2021 03:53:16 - INFO - volta.train_utils -   [GQA]: iter 6800 Ep: 0.46 loss 0.260 score 0.077 lr 2.29469e-06 
12/03/2021 03:53:48 - INFO - volta.train_utils -   [GQA]: iter 6880 Ep: 0.47 loss 0.259 score 0.082 lr 2.32184e-06 
12/03/2021 03:54:21 - INFO - volta.train_utils -   [GQA]: iter 6960 Ep: 0.47 loss 0.259 score 0.079 lr 2.34899e-06 
12/03/2021 03:54:54 - INFO - volta.train_utils -   [GQA]: iter 7040 Ep: 0.48 loss 0.254 score 0.082 lr 2.37614e-06 
12/03/2021 03:55:27 - INFO - volta.train_utils -   [GQA]: iter 7120 Ep: 0.48 loss 0.247 score 0.082 lr 2.40328e-06 
12/03/2021 03:56:00 - INFO - volta.train_utils -   [GQA]: iter 7200 Ep: 0.49 loss 0.242 score 0.087 lr 2.43043e-06 
12/03/2021 03:56:32 - INFO - volta.train_utils -   [GQA]: iter 7280 Ep: 0.49 loss 0.244 score 0.083 lr 2.45758e-06 
12/03/2021 03:57:05 - INFO - volta.train_utils -   [GQA]: iter 7360 Ep: 0.50 loss 0.250 score 0.085 lr 2.48473e-06 
12/03/2021 03:57:38 - INFO - volta.train_utils -   [GQA]: iter 7440 Ep: 0.50 loss 0.230 score 0.090 lr 2.51188e-06 
12/03/2021 03:58:11 - INFO - volta.train_utils -   [GQA]: iter 7520 Ep: 0.51 loss 0.242 score 0.087 lr 2.53903e-06 
12/03/2021 03:58:43 - INFO - volta.train_utils -   [GQA]: iter 7600 Ep: 0.52 loss 0.239 score 0.084 lr 2.56617e-06 
12/03/2021 03:59:16 - INFO - volta.train_utils -   [GQA]: iter 7680 Ep: 0.52 loss 0.240 score 0.087 lr 2.59332e-06 
12/03/2021 03:59:48 - INFO - volta.train_utils -   [GQA]: iter 7760 Ep: 0.53 loss 0.231 score 0.090 lr 2.62047e-06 
12/03/2021 04:00:21 - INFO - volta.train_utils -   [GQA]: iter 7840 Ep: 0.53 loss 0.223 score 0.088 lr 2.64762e-06 
12/03/2021 04:00:54 - INFO - volta.train_utils -   [GQA]: iter 7920 Ep: 0.54 loss 0.225 score 0.090 lr 2.67477e-06 
12/03/2021 04:01:27 - INFO - volta.train_utils -   [GQA]: iter 8000 Ep: 0.54 loss 0.226 score 0.094 lr 2.70191e-06 
12/03/2021 04:02:00 - INFO - volta.train_utils -   [GQA]: iter 8080 Ep: 0.55 loss 0.228 score 0.091 lr 2.72906e-06 
12/03/2021 04:02:34 - INFO - volta.train_utils -   [GQA]: iter 8160 Ep: 0.55 loss 0.228 score 0.093 lr 2.75621e-06 
12/03/2021 04:03:08 - INFO - volta.train_utils -   [GQA]: iter 8240 Ep: 0.56 loss 0.215 score 0.099 lr 2.78336e-06 
12/03/2021 04:03:41 - INFO - volta.train_utils -   [GQA]: iter 8320 Ep: 0.56 loss 0.226 score 0.094 lr 2.81051e-06 
12/03/2021 04:04:14 - INFO - volta.train_utils -   [GQA]: iter 8400 Ep: 0.57 loss 0.226 score 0.096 lr 2.83765e-06 
12/03/2021 04:04:47 - INFO - volta.train_utils -   [GQA]: iter 8480 Ep: 0.58 loss 0.217 score 0.094 lr 2.8648e-06 
12/03/2021 04:05:19 - INFO - volta.train_utils -   [GQA]: iter 8560 Ep: 0.58 loss 0.212 score 0.100 lr 2.89195e-06 
12/03/2021 04:05:52 - INFO - volta.train_utils -   [GQA]: iter 8640 Ep: 0.59 loss 0.210 score 0.101 lr 2.9191e-06 
12/03/2021 04:06:24 - INFO - volta.train_utils -   [GQA]: iter 8720 Ep: 0.59 loss 0.213 score 0.096 lr 2.94625e-06 
12/03/2021 04:06:57 - INFO - volta.train_utils -   [GQA]: iter 8800 Ep: 0.60 loss 0.202 score 0.101 lr 2.97339e-06 
12/03/2021 04:07:30 - INFO - volta.train_utils -   [GQA]: iter 8880 Ep: 0.60 loss 0.210 score 0.097 lr 3.00054e-06 
12/03/2021 04:08:02 - INFO - volta.train_utils -   [GQA]: iter 8960 Ep: 0.61 loss 0.206 score 0.102 lr 3.02769e-06 
12/03/2021 04:08:35 - INFO - volta.train_utils -   [GQA]: iter 9040 Ep: 0.61 loss 0.207 score 0.100 lr 3.05484e-06 
12/03/2021 04:09:08 - INFO - volta.train_utils -   [GQA]: iter 9120 Ep: 0.62 loss 0.203 score 0.102 lr 3.08199e-06 
12/03/2021 04:09:40 - INFO - volta.train_utils -   [GQA]: iter 9200 Ep: 0.62 loss 0.207 score 0.104 lr 3.10914e-06 
12/03/2021 04:10:15 - INFO - volta.train_utils -   [GQA]: iter 9280 Ep: 0.63 loss 0.207 score 0.102 lr 3.13628e-06 
12/03/2021 04:10:48 - INFO - volta.train_utils -   [GQA]: iter 9360 Ep: 0.64 loss 0.208 score 0.101 lr 3.16343e-06 
12/03/2021 04:11:22 - INFO - volta.train_utils -   [GQA]: iter 9440 Ep: 0.64 loss 0.201 score 0.102 lr 3.19058e-06 
12/03/2021 04:11:55 - INFO - volta.train_utils -   [GQA]: iter 9520 Ep: 0.65 loss 0.191 score 0.108 lr 3.21773e-06 
12/03/2021 04:12:27 - INFO - volta.train_utils -   [GQA]: iter 9600 Ep: 0.65 loss 0.192 score 0.108 lr 3.24488e-06 
12/03/2021 04:13:00 - INFO - volta.train_utils -   [GQA]: iter 9680 Ep: 0.66 loss 0.196 score 0.106 lr 3.27202e-06 
12/03/2021 04:13:33 - INFO - volta.train_utils -   [GQA]: iter 9760 Ep: 0.66 loss 0.195 score 0.106 lr 3.29917e-06 
12/03/2021 04:14:06 - INFO - volta.train_utils -   [GQA]: iter 9840 Ep: 0.67 loss 0.201 score 0.101 lr 3.32632e-06 
12/03/2021 04:14:38 - INFO - volta.train_utils -   [GQA]: iter 9920 Ep: 0.67 loss 0.194 score 0.106 lr 3.35347e-06 
12/03/2021 04:15:11 - INFO - volta.train_utils -   [GQA]: iter 10000 Ep: 0.68 loss 0.189 score 0.109 lr 3.38062e-06 
12/03/2021 04:15:44 - INFO - volta.train_utils -   [GQA]: iter 10080 Ep: 0.68 loss 0.192 score 0.107 lr 3.40776e-06 
12/03/2021 04:16:17 - INFO - volta.train_utils -   [GQA]: iter 10160 Ep: 0.69 loss 0.188 score 0.107 lr 3.43491e-06 
12/03/2021 04:16:49 - INFO - volta.train_utils -   [GQA]: iter 10240 Ep: 0.69 loss 0.191 score 0.109 lr 3.46206e-06 
12/03/2021 04:17:22 - INFO - volta.train_utils -   [GQA]: iter 10320 Ep: 0.70 loss 0.190 score 0.107 lr 3.48921e-06 
12/03/2021 04:17:55 - INFO - volta.train_utils -   [GQA]: iter 10400 Ep: 0.71 loss 0.192 score 0.108 lr 3.51636e-06 
12/03/2021 04:18:27 - INFO - volta.train_utils -   [GQA]: iter 10480 Ep: 0.71 loss 0.183 score 0.112 lr 3.5435e-06 
12/03/2021 04:19:01 - INFO - volta.train_utils -   [GQA]: iter 10560 Ep: 0.72 loss 0.183 score 0.114 lr 3.57065e-06 
12/03/2021 04:19:37 - INFO - volta.train_utils -   [GQA]: iter 10640 Ep: 0.72 loss 0.188 score 0.112 lr 3.5978e-06 
12/03/2021 04:20:10 - INFO - volta.train_utils -   [GQA]: iter 10720 Ep: 0.73 loss 0.181 score 0.112 lr 3.62495e-06 
12/03/2021 04:20:43 - INFO - volta.train_utils -   [GQA]: iter 10800 Ep: 0.73 loss 0.184 score 0.111 lr 3.6521e-06 
12/03/2021 04:21:16 - INFO - volta.train_utils -   [GQA]: iter 10880 Ep: 0.74 loss 0.189 score 0.108 lr 3.67925e-06 
12/03/2021 04:21:49 - INFO - volta.train_utils -   [GQA]: iter 10960 Ep: 0.74 loss 0.185 score 0.115 lr 3.70639e-06 
12/03/2021 04:22:21 - INFO - volta.train_utils -   [GQA]: iter 11040 Ep: 0.75 loss 0.184 score 0.110 lr 3.73354e-06 
12/03/2021 04:22:54 - INFO - volta.train_utils -   [GQA]: iter 11120 Ep: 0.75 loss 0.182 score 0.115 lr 3.76069e-06 
12/03/2021 04:23:27 - INFO - volta.train_utils -   [GQA]: iter 11200 Ep: 0.76 loss 0.177 score 0.110 lr 3.78784e-06 
12/03/2021 04:23:59 - INFO - volta.train_utils -   [GQA]: iter 11280 Ep: 0.77 loss 0.181 score 0.109 lr 3.81499e-06 
12/03/2021 04:24:32 - INFO - volta.train_utils -   [GQA]: iter 11360 Ep: 0.77 loss 0.181 score 0.113 lr 3.84213e-06 
12/03/2021 04:25:05 - INFO - volta.train_utils -   [GQA]: iter 11440 Ep: 0.78 loss 0.183 score 0.116 lr 3.86928e-06 
12/03/2021 04:25:38 - INFO - volta.train_utils -   [GQA]: iter 11520 Ep: 0.78 loss 0.175 score 0.113 lr 3.89643e-06 
12/03/2021 04:26:11 - INFO - volta.train_utils -   [GQA]: iter 11600 Ep: 0.79 loss 0.179 score 0.110 lr 3.92358e-06 
12/03/2021 04:26:46 - INFO - volta.train_utils -   [GQA]: iter 11680 Ep: 0.79 loss 0.178 score 0.112 lr 3.95073e-06 
12/03/2021 04:27:18 - INFO - volta.train_utils -   [GQA]: iter 11760 Ep: 0.80 loss 0.178 score 0.115 lr 3.97787e-06 
12/03/2021 04:27:51 - INFO - volta.train_utils -   [GQA]: iter 11840 Ep: 0.80 loss 0.162 score 0.116 lr 4.00502e-06 
12/03/2021 04:28:24 - INFO - volta.train_utils -   [GQA]: iter 11920 Ep: 0.81 loss 0.180 score 0.113 lr 4.03217e-06 
12/03/2021 04:28:57 - INFO - volta.train_utils -   [GQA]: iter 12000 Ep: 0.81 loss 0.176 score 0.112 lr 4.05932e-06 
12/03/2021 04:29:29 - INFO - volta.train_utils -   [GQA]: iter 12080 Ep: 0.82 loss 0.174 score 0.113 lr 4.08647e-06 
12/03/2021 04:30:06 - INFO - volta.train_utils -   [GQA]: iter 12160 Ep: 0.83 loss 0.177 score 0.115 lr 4.11361e-06 
12/03/2021 04:30:39 - INFO - volta.train_utils -   [GQA]: iter 12240 Ep: 0.83 loss 0.172 score 0.114 lr 4.14076e-06 
12/03/2021 04:31:12 - INFO - volta.train_utils -   [GQA]: iter 12320 Ep: 0.84 loss 0.171 score 0.118 lr 4.16791e-06 
12/03/2021 04:31:44 - INFO - volta.train_utils -   [GQA]: iter 12400 Ep: 0.84 loss 0.167 score 0.116 lr 4.19506e-06 
12/03/2021 04:32:20 - INFO - volta.train_utils -   [GQA]: iter 12480 Ep: 0.85 loss 0.175 score 0.111 lr 4.22221e-06 
12/03/2021 04:32:52 - INFO - volta.train_utils -   [GQA]: iter 12560 Ep: 0.85 loss 0.169 score 0.117 lr 4.24936e-06 
12/03/2021 04:33:25 - INFO - volta.train_utils -   [GQA]: iter 12640 Ep: 0.86 loss 0.172 score 0.117 lr 4.2765e-06 
12/03/2021 04:33:58 - INFO - volta.train_utils -   [GQA]: iter 12720 Ep: 0.86 loss 0.180 score 0.115 lr 4.30365e-06 
12/03/2021 04:34:30 - INFO - volta.train_utils -   [GQA]: iter 12800 Ep: 0.87 loss 0.170 score 0.116 lr 4.3308e-06 
12/03/2021 04:35:03 - INFO - volta.train_utils -   [GQA]: iter 12880 Ep: 0.87 loss 0.169 score 0.119 lr 4.35795e-06 
12/03/2021 04:35:36 - INFO - volta.train_utils -   [GQA]: iter 12960 Ep: 0.88 loss 0.173 score 0.115 lr 4.3851e-06 
12/03/2021 04:36:08 - INFO - volta.train_utils -   [GQA]: iter 13040 Ep: 0.89 loss 0.169 score 0.113 lr 4.41224e-06 
12/03/2021 04:36:41 - INFO - volta.train_utils -   [GQA]: iter 13120 Ep: 0.89 loss 0.170 score 0.119 lr 4.43939e-06 
12/03/2021 04:37:14 - INFO - volta.train_utils -   [GQA]: iter 13200 Ep: 0.90 loss 0.177 score 0.114 lr 4.46654e-06 
12/03/2021 04:37:46 - INFO - volta.train_utils -   [GQA]: iter 13280 Ep: 0.90 loss 0.166 score 0.118 lr 4.49369e-06 
12/03/2021 04:38:21 - INFO - volta.train_utils -   [GQA]: iter 13360 Ep: 0.91 loss 0.165 score 0.114 lr 4.52084e-06 
12/03/2021 04:38:54 - INFO - volta.train_utils -   [GQA]: iter 13440 Ep: 0.91 loss 0.165 score 0.117 lr 4.54798e-06 
12/03/2021 04:39:27 - INFO - volta.train_utils -   [GQA]: iter 13520 Ep: 0.92 loss 0.170 score 0.116 lr 4.57513e-06 
12/03/2021 04:39:59 - INFO - volta.train_utils -   [GQA]: iter 13600 Ep: 0.92 loss 0.168 score 0.115 lr 4.60228e-06 
12/03/2021 04:40:32 - INFO - volta.train_utils -   [GQA]: iter 13680 Ep: 0.93 loss 0.173 score 0.117 lr 4.62943e-06 
12/03/2021 04:41:07 - INFO - volta.train_utils -   [GQA]: iter 13760 Ep: 0.93 loss 0.164 score 0.117 lr 4.65658e-06 
12/03/2021 04:41:40 - INFO - volta.train_utils -   [GQA]: iter 13840 Ep: 0.94 loss 0.161 score 0.120 lr 4.68372e-06 
12/03/2021 04:42:12 - INFO - volta.train_utils -   [GQA]: iter 13920 Ep: 0.94 loss 0.160 score 0.116 lr 4.71087e-06 
12/03/2021 04:42:45 - INFO - volta.train_utils -   [GQA]: iter 14000 Ep: 0.95 loss 0.163 score 0.118 lr 4.73802e-06 
12/03/2021 04:43:18 - INFO - volta.train_utils -   [GQA]: iter 14080 Ep: 0.96 loss 0.163 score 0.116 lr 4.76517e-06 
12/03/2021 04:43:52 - INFO - volta.train_utils -   [GQA]: iter 14160 Ep: 0.96 loss 0.164 score 0.117 lr 4.79232e-06 
12/03/2021 04:44:24 - INFO - volta.train_utils -   [GQA]: iter 14240 Ep: 0.97 loss 0.163 score 0.118 lr 4.81947e-06 
12/03/2021 04:44:57 - INFO - volta.train_utils -   [GQA]: iter 14320 Ep: 0.97 loss 0.160 score 0.121 lr 4.84661e-06 
12/03/2021 04:45:29 - INFO - volta.train_utils -   [GQA]: iter 14400 Ep: 0.98 loss 0.166 score 0.115 lr 4.87376e-06 
12/03/2021 04:46:02 - INFO - volta.train_utils -   [GQA]: iter 14480 Ep: 0.98 loss 0.159 score 0.120 lr 4.90091e-06 
12/03/2021 04:46:35 - INFO - volta.train_utils -   [GQA]: iter 14560 Ep: 0.99 loss 0.160 score 0.117 lr 4.92806e-06 
12/03/2021 04:47:07 - INFO - volta.train_utils -   [GQA]: iter 14640 Ep: 0.99 loss 0.159 score 0.121 lr 4.95521e-06 
12/03/2021 04:47:40 - INFO - volta.train_utils -   [GQA]: iter 14720 Ep: 1.00 loss 0.163 score 0.119 lr 4.98235e-06 
12/03/2021 04:52:48 - INFO - volta.train_utils -   Eval task TASK15 on iteration 14732 
12/03/2021 04:52:48 - INFO - volta.train_utils -   Validation [GQA]: loss 2.526 score 47.353 
12/03/2021 04:52:48 - INFO - __main__ -   ** ** * Saving model * ** ** 
Epoch:  20%|██        | 1/5 [1:46:21<7:05:25, 6381.43s/it]12/03/2021 04:53:38 - INFO - volta.train_utils -   [GQA]: iter 14812 Ep: 1.01 loss 0.161 score 0.119 lr 5.01154e-06 
12/03/2021 04:54:10 - INFO - volta.train_utils -   [GQA]: iter 14892 Ep: 1.01 loss 0.160 score 0.120 lr 5.04072e-06 
12/03/2021 04:54:43 - INFO - volta.train_utils -   [GQA]: iter 14972 Ep: 1.02 loss 0.151 score 0.123 lr 5.06787e-06 
12/03/2021 04:55:15 - INFO - volta.train_utils -   [GQA]: iter 15052 Ep: 1.02 loss 0.160 score 0.115 lr 5.09502e-06 
12/03/2021 04:55:48 - INFO - volta.train_utils -   [GQA]: iter 15132 Ep: 1.03 loss 0.159 score 0.116 lr 5.12217e-06 
12/03/2021 04:56:20 - INFO - volta.train_utils -   [GQA]: iter 15212 Ep: 1.03 loss 0.153 score 0.118 lr 5.14931e-06 
12/03/2021 04:56:53 - INFO - volta.train_utils -   [GQA]: iter 15292 Ep: 1.04 loss 0.162 score 0.119 lr 5.17646e-06 
12/03/2021 04:57:25 - INFO - volta.train_utils -   [GQA]: iter 15372 Ep: 1.04 loss 0.154 score 0.122 lr 5.20361e-06 
12/03/2021 04:57:58 - INFO - volta.train_utils -   [GQA]: iter 15452 Ep: 1.05 loss 0.164 score 0.120 lr 5.23076e-06 
12/03/2021 04:58:30 - INFO - volta.train_utils -   [GQA]: iter 15532 Ep: 1.05 loss 0.158 score 0.116 lr 5.25791e-06 
12/03/2021 04:59:04 - INFO - volta.train_utils -   [GQA]: iter 15612 Ep: 1.06 loss 0.157 score 0.119 lr 5.28505e-06 
12/03/2021 04:59:36 - INFO - volta.train_utils -   [GQA]: iter 15692 Ep: 1.07 loss 0.160 score 0.120 lr 5.3122e-06 
12/03/2021 05:00:09 - INFO - volta.train_utils -   [GQA]: iter 15772 Ep: 1.07 loss 0.157 score 0.119 lr 5.33935e-06 
12/03/2021 05:00:46 - INFO - volta.train_utils -   [GQA]: iter 15852 Ep: 1.08 loss 0.149 score 0.120 lr 5.3665e-06 
12/03/2021 05:01:18 - INFO - volta.train_utils -   [GQA]: iter 15932 Ep: 1.08 loss 0.160 score 0.121 lr 5.39365e-06 
12/03/2021 05:01:51 - INFO - volta.train_utils -   [GQA]: iter 16012 Ep: 1.09 loss 0.155 score 0.117 lr 5.4208e-06 
12/03/2021 05:02:23 - INFO - volta.train_utils -   [GQA]: iter 16092 Ep: 1.09 loss 0.156 score 0.119 lr 5.44794e-06 
12/03/2021 05:02:56 - INFO - volta.train_utils -   [GQA]: iter 16172 Ep: 1.10 loss 0.150 score 0.118 lr 5.47509e-06 
12/03/2021 05:03:28 - INFO - volta.train_utils -   [GQA]: iter 16252 Ep: 1.10 loss 0.149 score 0.125 lr 5.50224e-06 
12/03/2021 05:04:01 - INFO - volta.train_utils -   [GQA]: iter 16332 Ep: 1.11 loss 0.158 score 0.123 lr 5.52939e-06 
12/03/2021 05:04:33 - INFO - volta.train_utils -   [GQA]: iter 16412 Ep: 1.11 loss 0.160 score 0.119 lr 5.55654e-06 
12/03/2021 05:05:06 - INFO - volta.train_utils -   [GQA]: iter 16492 Ep: 1.12 loss 0.154 score 0.118 lr 5.58368e-06 
12/03/2021 05:05:39 - INFO - volta.train_utils -   [GQA]: iter 16572 Ep: 1.12 loss 0.155 score 0.121 lr 5.61083e-06 
12/03/2021 05:06:12 - INFO - volta.train_utils -   [GQA]: iter 16652 Ep: 1.13 loss 0.154 score 0.123 lr 5.63798e-06 
12/03/2021 05:06:44 - INFO - volta.train_utils -   [GQA]: iter 16732 Ep: 1.14 loss 0.153 score 0.123 lr 5.66513e-06 
12/03/2021 05:07:17 - INFO - volta.train_utils -   [GQA]: iter 16812 Ep: 1.14 loss 0.155 score 0.121 lr 5.69228e-06 
12/03/2021 05:07:49 - INFO - volta.train_utils -   [GQA]: iter 16892 Ep: 1.15 loss 0.156 score 0.121 lr 5.71942e-06 
12/03/2021 05:08:22 - INFO - volta.train_utils -   [GQA]: iter 16972 Ep: 1.15 loss 0.151 score 0.122 lr 5.74657e-06 
12/03/2021 05:08:56 - INFO - volta.train_utils -   [GQA]: iter 17052 Ep: 1.16 loss 0.157 score 0.121 lr 5.77372e-06 
12/03/2021 05:09:28 - INFO - volta.train_utils -   [GQA]: iter 17132 Ep: 1.16 loss 0.145 score 0.122 lr 5.80087e-06 
12/03/2021 05:10:01 - INFO - volta.train_utils -   [GQA]: iter 17212 Ep: 1.17 loss 0.149 score 0.121 lr 5.82802e-06 
12/03/2021 05:10:33 - INFO - volta.train_utils -   [GQA]: iter 17292 Ep: 1.17 loss 0.156 score 0.124 lr 5.85516e-06 
12/03/2021 05:11:05 - INFO - volta.train_utils -   [GQA]: iter 17372 Ep: 1.18 loss 0.153 score 0.122 lr 5.88231e-06 
12/03/2021 05:11:38 - INFO - volta.train_utils -   [GQA]: iter 17452 Ep: 1.18 loss 0.155 score 0.125 lr 5.90946e-06 
12/03/2021 05:12:11 - INFO - volta.train_utils -   [GQA]: iter 17532 Ep: 1.19 loss 0.149 score 0.130 lr 5.93661e-06 
12/03/2021 05:12:46 - INFO - volta.train_utils -   [GQA]: iter 17612 Ep: 1.20 loss 0.155 score 0.124 lr 5.96376e-06 
12/03/2021 05:13:22 - INFO - volta.train_utils -   [GQA]: iter 17692 Ep: 1.20 loss 0.150 score 0.127 lr 5.99091e-06 
12/03/2021 05:13:55 - INFO - volta.train_utils -   [GQA]: iter 17772 Ep: 1.21 loss 0.147 score 0.128 lr 6.01805e-06 
12/03/2021 05:14:27 - INFO - volta.train_utils -   [GQA]: iter 17852 Ep: 1.21 loss 0.150 score 0.127 lr 6.0452e-06 
12/03/2021 05:14:59 - INFO - volta.train_utils -   [GQA]: iter 17932 Ep: 1.22 loss 0.149 score 0.129 lr 6.07235e-06 
12/03/2021 05:15:33 - INFO - volta.train_utils -   [GQA]: iter 18012 Ep: 1.22 loss 0.147 score 0.129 lr 6.0995e-06 
12/03/2021 05:16:06 - INFO - volta.train_utils -   [GQA]: iter 18092 Ep: 1.23 loss 0.150 score 0.132 lr 6.12665e-06 
12/03/2021 05:16:38 - INFO - volta.train_utils -   [GQA]: iter 18172 Ep: 1.23 loss 0.145 score 0.126 lr 6.15379e-06 
12/03/2021 05:17:11 - INFO - volta.train_utils -   [GQA]: iter 18252 Ep: 1.24 loss 0.152 score 0.124 lr 6.18094e-06 
12/03/2021 05:17:43 - INFO - volta.train_utils -   [GQA]: iter 18332 Ep: 1.24 loss 0.139 score 0.128 lr 6.20809e-06 
12/03/2021 05:18:16 - INFO - volta.train_utils -   [GQA]: iter 18412 Ep: 1.25 loss 0.146 score 0.129 lr 6.23524e-06 
12/03/2021 05:18:49 - INFO - volta.train_utils -   [GQA]: iter 18492 Ep: 1.26 loss 0.146 score 0.130 lr 6.26239e-06 
12/03/2021 05:19:21 - INFO - volta.train_utils -   [GQA]: iter 18572 Ep: 1.26 loss 0.148 score 0.122 lr 6.28953e-06 
12/03/2021 05:19:53 - INFO - volta.train_utils -   [GQA]: iter 18652 Ep: 1.27 loss 0.152 score 0.126 lr 6.31668e-06 
12/03/2021 05:20:26 - INFO - volta.train_utils -   [GQA]: iter 18732 Ep: 1.27 loss 0.156 score 0.128 lr 6.34383e-06 
12/03/2021 05:20:58 - INFO - volta.train_utils -   [GQA]: iter 18812 Ep: 1.28 loss 0.146 score 0.126 lr 6.37098e-06 
12/03/2021 05:21:32 - INFO - volta.train_utils -   [GQA]: iter 18892 Ep: 1.28 loss 0.152 score 0.124 lr 6.39813e-06 
12/03/2021 05:22:04 - INFO - volta.train_utils -   [GQA]: iter 18972 Ep: 1.29 loss 0.142 score 0.128 lr 6.42527e-06 
12/03/2021 05:22:37 - INFO - volta.train_utils -   [GQA]: iter 19052 Ep: 1.29 loss 0.141 score 0.127 lr 6.45242e-06 
12/03/2021 05:23:10 - INFO - volta.train_utils -   [GQA]: iter 19132 Ep: 1.30 loss 0.156 score 0.126 lr 6.47957e-06 
12/03/2021 05:23:42 - INFO - volta.train_utils -   [GQA]: iter 19212 Ep: 1.30 loss 0.139 score 0.129 lr 6.50672e-06 
12/03/2021 05:24:14 - INFO - volta.train_utils -   [GQA]: iter 19292 Ep: 1.31 loss 0.135 score 0.132 lr 6.53387e-06 
12/03/2021 05:24:47 - INFO - volta.train_utils -   [GQA]: iter 19372 Ep: 1.31 loss 0.145 score 0.129 lr 6.56102e-06 
12/03/2021 05:25:22 - INFO - volta.train_utils -   [GQA]: iter 19452 Ep: 1.32 loss 0.145 score 0.133 lr 6.58816e-06 
12/03/2021 05:25:58 - INFO - volta.train_utils -   [GQA]: iter 19532 Ep: 1.33 loss 0.146 score 0.126 lr 6.61531e-06 
12/03/2021 05:26:30 - INFO - volta.train_utils -   [GQA]: iter 19612 Ep: 1.33 loss 0.148 score 0.124 lr 6.64246e-06 
12/03/2021 05:27:03 - INFO - volta.train_utils -   [GQA]: iter 19692 Ep: 1.34 loss 0.149 score 0.127 lr 6.66961e-06 
12/03/2021 05:27:36 - INFO - volta.train_utils -   [GQA]: iter 19772 Ep: 1.34 loss 0.137 score 0.131 lr 6.69676e-06 
12/03/2021 05:28:08 - INFO - volta.train_utils -   [GQA]: iter 19852 Ep: 1.35 loss 0.153 score 0.126 lr 6.7239e-06 
12/03/2021 05:28:40 - INFO - volta.train_utils -   [GQA]: iter 19932 Ep: 1.35 loss 0.136 score 0.128 lr 6.75105e-06 
12/03/2021 05:29:12 - INFO - volta.train_utils -   [GQA]: iter 20012 Ep: 1.36 loss 0.143 score 0.130 lr 6.7782e-06 
12/03/2021 05:29:45 - INFO - volta.train_utils -   [GQA]: iter 20092 Ep: 1.36 loss 0.146 score 0.126 lr 6.80535e-06 
12/03/2021 05:30:18 - INFO - volta.train_utils -   [GQA]: iter 20172 Ep: 1.37 loss 0.146 score 0.128 lr 6.8325e-06 
12/03/2021 05:30:51 - INFO - volta.train_utils -   [GQA]: iter 20252 Ep: 1.37 loss 0.147 score 0.128 lr 6.85964e-06 
12/03/2021 05:31:23 - INFO - volta.train_utils -   [GQA]: iter 20332 Ep: 1.38 loss 0.139 score 0.129 lr 6.88679e-06 
12/03/2021 05:31:56 - INFO - volta.train_utils -   [GQA]: iter 20412 Ep: 1.39 loss 0.146 score 0.131 lr 6.91394e-06 
12/03/2021 05:32:28 - INFO - volta.train_utils -   [GQA]: iter 20492 Ep: 1.39 loss 0.143 score 0.133 lr 6.94109e-06 
12/03/2021 05:33:02 - INFO - volta.train_utils -   [GQA]: iter 20572 Ep: 1.40 loss 0.132 score 0.129 lr 6.96824e-06 
12/03/2021 05:33:35 - INFO - volta.train_utils -   [GQA]: iter 20652 Ep: 1.40 loss 0.138 score 0.133 lr 6.99538e-06 
12/03/2021 05:34:08 - INFO - volta.train_utils -   [GQA]: iter 20732 Ep: 1.41 loss 0.138 score 0.130 lr 7.02253e-06 
12/03/2021 05:34:41 - INFO - volta.train_utils -   [GQA]: iter 20812 Ep: 1.41 loss 0.143 score 0.131 lr 7.04968e-06 
12/03/2021 05:35:14 - INFO - volta.train_utils -   [GQA]: iter 20892 Ep: 1.42 loss 0.135 score 0.135 lr 7.07683e-06 
12/03/2021 05:35:47 - INFO - volta.train_utils -   [GQA]: iter 20972 Ep: 1.42 loss 0.138 score 0.131 lr 7.10398e-06 
12/03/2021 05:36:21 - INFO - volta.train_utils -   [GQA]: iter 21052 Ep: 1.43 loss 0.142 score 0.134 lr 7.13113e-06 
12/03/2021 05:36:54 - INFO - volta.train_utils -   [GQA]: iter 21132 Ep: 1.43 loss 0.143 score 0.129 lr 7.15827e-06 
12/03/2021 05:37:27 - INFO - volta.train_utils -   [GQA]: iter 21212 Ep: 1.44 loss 0.143 score 0.128 lr 7.18542e-06 
12/03/2021 05:38:00 - INFO - volta.train_utils -   [GQA]: iter 21292 Ep: 1.45 loss 0.142 score 0.131 lr 7.21257e-06 
12/03/2021 05:38:33 - INFO - volta.train_utils -   [GQA]: iter 21372 Ep: 1.45 loss 0.136 score 0.133 lr 7.23972e-06 
12/03/2021 05:39:11 - INFO - volta.train_utils -   [GQA]: iter 21452 Ep: 1.46 loss 0.141 score 0.131 lr 7.26687e-06 
12/03/2021 05:39:44 - INFO - volta.train_utils -   [GQA]: iter 21532 Ep: 1.46 loss 0.139 score 0.131 lr 7.29401e-06 
12/03/2021 05:40:17 - INFO - volta.train_utils -   [GQA]: iter 21612 Ep: 1.47 loss 0.141 score 0.130 lr 7.32116e-06 
12/03/2021 05:40:49 - INFO - volta.train_utils -   [GQA]: iter 21692 Ep: 1.47 loss 0.142 score 0.132 lr 7.34831e-06 
12/03/2021 05:41:22 - INFO - volta.train_utils -   [GQA]: iter 21772 Ep: 1.48 loss 0.139 score 0.131 lr 7.37546e-06 
12/03/2021 05:41:55 - INFO - volta.train_utils -   [GQA]: iter 21852 Ep: 1.48 loss 0.138 score 0.128 lr 7.40261e-06 
12/03/2021 05:42:27 - INFO - volta.train_utils -   [GQA]: iter 21932 Ep: 1.49 loss 0.136 score 0.135 lr 7.42975e-06 
12/03/2021 05:43:00 - INFO - volta.train_utils -   [GQA]: iter 22012 Ep: 1.49 loss 0.142 score 0.132 lr 7.4569e-06 
12/03/2021 05:43:33 - INFO - volta.train_utils -   [GQA]: iter 22092 Ep: 1.50 loss 0.139 score 0.135 lr 7.48405e-06 
12/03/2021 05:44:05 - INFO - volta.train_utils -   [GQA]: iter 22172 Ep: 1.50 loss 0.140 score 0.132 lr 7.5112e-06 
12/03/2021 05:44:38 - INFO - volta.train_utils -   [GQA]: iter 22252 Ep: 1.51 loss 0.139 score 0.129 lr 7.53835e-06 
12/03/2021 05:45:12 - INFO - volta.train_utils -   [GQA]: iter 22332 Ep: 1.52 loss 0.148 score 0.131 lr 7.56549e-06 
12/03/2021 05:45:44 - INFO - volta.train_utils -   [GQA]: iter 22412 Ep: 1.52 loss 0.142 score 0.128 lr 7.59264e-06 
12/03/2021 05:46:17 - INFO - volta.train_utils -   [GQA]: iter 22492 Ep: 1.53 loss 0.143 score 0.129 lr 7.61979e-06 
12/03/2021 05:46:50 - INFO - volta.train_utils -   [GQA]: iter 22572 Ep: 1.53 loss 0.138 score 0.135 lr 7.64694e-06 
12/03/2021 05:47:22 - INFO - volta.train_utils -   [GQA]: iter 22652 Ep: 1.54 loss 0.136 score 0.135 lr 7.67409e-06 
12/03/2021 05:47:56 - INFO - volta.train_utils -   [GQA]: iter 22732 Ep: 1.54 loss 0.136 score 0.138 lr 7.70124e-06 
12/03/2021 05:48:29 - INFO - volta.train_utils -   [GQA]: iter 22812 Ep: 1.55 loss 0.133 score 0.131 lr 7.72838e-06 
12/03/2021 05:49:02 - INFO - volta.train_utils -   [GQA]: iter 22892 Ep: 1.55 loss 0.135 score 0.135 lr 7.75553e-06 
12/03/2021 05:49:34 - INFO - volta.train_utils -   [GQA]: iter 22972 Ep: 1.56 loss 0.131 score 0.139 lr 7.78268e-06 
12/03/2021 05:50:07 - INFO - volta.train_utils -   [GQA]: iter 23052 Ep: 1.56 loss 0.140 score 0.129 lr 7.80983e-06 
12/03/2021 05:50:41 - INFO - volta.train_utils -   [GQA]: iter 23132 Ep: 1.57 loss 0.144 score 0.130 lr 7.83698e-06 
12/03/2021 05:51:14 - INFO - volta.train_utils -   [GQA]: iter 23212 Ep: 1.58 loss 0.143 score 0.137 lr 7.86412e-06 
12/03/2021 05:51:50 - INFO - volta.train_utils -   [GQA]: iter 23292 Ep: 1.58 loss 0.133 score 0.132 lr 7.89127e-06 
12/03/2021 05:52:26 - INFO - volta.train_utils -   [GQA]: iter 23372 Ep: 1.59 loss 0.130 score 0.133 lr 7.91842e-06 
12/03/2021 05:52:59 - INFO - volta.train_utils -   [GQA]: iter 23452 Ep: 1.59 loss 0.131 score 0.135 lr 7.94557e-06 
12/03/2021 05:53:32 - INFO - volta.train_utils -   [GQA]: iter 23532 Ep: 1.60 loss 0.127 score 0.136 lr 7.97272e-06 
12/03/2021 05:54:06 - INFO - volta.train_utils -   [GQA]: iter 23612 Ep: 1.60 loss 0.140 score 0.132 lr 7.99986e-06 
12/03/2021 05:54:39 - INFO - volta.train_utils -   [GQA]: iter 23692 Ep: 1.61 loss 0.141 score 0.134 lr 8.02701e-06 
12/03/2021 05:55:12 - INFO - volta.train_utils -   [GQA]: iter 23772 Ep: 1.61 loss 0.126 score 0.134 lr 8.05416e-06 
12/03/2021 05:55:44 - INFO - volta.train_utils -   [GQA]: iter 23852 Ep: 1.62 loss 0.137 score 0.131 lr 8.08131e-06 
12/03/2021 05:56:17 - INFO - volta.train_utils -   [GQA]: iter 23932 Ep: 1.62 loss 0.137 score 0.136 lr 8.10846e-06 
12/03/2021 05:56:51 - INFO - volta.train_utils -   [GQA]: iter 24012 Ep: 1.63 loss 0.141 score 0.130 lr 8.1356e-06 
12/03/2021 05:57:24 - INFO - volta.train_utils -   [GQA]: iter 24092 Ep: 1.64 loss 0.132 score 0.133 lr 8.16275e-06 
12/03/2021 05:57:56 - INFO - volta.train_utils -   [GQA]: iter 24172 Ep: 1.64 loss 0.139 score 0.131 lr 8.1899e-06 
12/03/2021 05:58:29 - INFO - volta.train_utils -   [GQA]: iter 24252 Ep: 1.65 loss 0.128 score 0.140 lr 8.21705e-06 
12/03/2021 05:59:02 - INFO - volta.train_utils -   [GQA]: iter 24332 Ep: 1.65 loss 0.134 score 0.133 lr 8.2442e-06 
12/03/2021 05:59:34 - INFO - volta.train_utils -   [GQA]: iter 24412 Ep: 1.66 loss 0.135 score 0.134 lr 8.27135e-06 
12/03/2021 06:00:07 - INFO - volta.train_utils -   [GQA]: iter 24492 Ep: 1.66 loss 0.137 score 0.135 lr 8.29849e-06 
12/03/2021 06:00:39 - INFO - volta.train_utils -   [GQA]: iter 24572 Ep: 1.67 loss 0.135 score 0.132 lr 8.32564e-06 
12/03/2021 06:01:12 - INFO - volta.train_utils -   [GQA]: iter 24652 Ep: 1.67 loss 0.134 score 0.140 lr 8.35279e-06 
12/03/2021 06:01:45 - INFO - volta.train_utils -   [GQA]: iter 24732 Ep: 1.68 loss 0.138 score 0.135 lr 8.37994e-06 
12/03/2021 06:02:18 - INFO - volta.train_utils -   [GQA]: iter 24812 Ep: 1.68 loss 0.135 score 0.134 lr 8.40709e-06 
12/03/2021 06:02:51 - INFO - volta.train_utils -   [GQA]: iter 24892 Ep: 1.69 loss 0.133 score 0.133 lr 8.43423e-06 
12/03/2021 06:03:24 - INFO - volta.train_utils -   [GQA]: iter 24972 Ep: 1.69 loss 0.139 score 0.139 lr 8.46138e-06 
12/03/2021 06:03:57 - INFO - volta.train_utils -   [GQA]: iter 25052 Ep: 1.70 loss 0.142 score 0.134 lr 8.48853e-06 
12/03/2021 06:04:31 - INFO - volta.train_utils -   [GQA]: iter 25132 Ep: 1.71 loss 0.140 score 0.132 lr 8.51568e-06 
12/03/2021 06:05:05 - INFO - volta.train_utils -   [GQA]: iter 25212 Ep: 1.71 loss 0.128 score 0.137 lr 8.54283e-06 
12/03/2021 06:05:40 - INFO - volta.train_utils -   [GQA]: iter 25292 Ep: 1.72 loss 0.127 score 0.140 lr 8.56997e-06 
12/03/2021 06:06:12 - INFO - volta.train_utils -   [GQA]: iter 25372 Ep: 1.72 loss 0.122 score 0.140 lr 8.59712e-06 
12/03/2021 06:06:45 - INFO - volta.train_utils -   [GQA]: iter 25452 Ep: 1.73 loss 0.129 score 0.137 lr 8.62427e-06 
12/03/2021 06:07:18 - INFO - volta.train_utils -   [GQA]: iter 25532 Ep: 1.73 loss 0.132 score 0.139 lr 8.65142e-06 
12/03/2021 06:07:51 - INFO - volta.train_utils -   [GQA]: iter 25612 Ep: 1.74 loss 0.128 score 0.135 lr 8.67857e-06 
12/03/2021 06:08:24 - INFO - volta.train_utils -   [GQA]: iter 25692 Ep: 1.74 loss 0.128 score 0.136 lr 8.70571e-06 
12/03/2021 06:08:56 - INFO - volta.train_utils -   [GQA]: iter 25772 Ep: 1.75 loss 0.129 score 0.136 lr 8.73286e-06 
12/03/2021 06:09:29 - INFO - volta.train_utils -   [GQA]: iter 25852 Ep: 1.75 loss 0.126 score 0.137 lr 8.76001e-06 
12/03/2021 06:10:02 - INFO - volta.train_utils -   [GQA]: iter 25932 Ep: 1.76 loss 0.136 score 0.134 lr 8.78716e-06 
12/03/2021 06:10:35 - INFO - volta.train_utils -   [GQA]: iter 26012 Ep: 1.77 loss 0.129 score 0.135 lr 8.81431e-06 
12/03/2021 06:11:08 - INFO - volta.train_utils -   [GQA]: iter 26092 Ep: 1.77 loss 0.128 score 0.138 lr 8.84146e-06 
12/03/2021 06:11:41 - INFO - volta.train_utils -   [GQA]: iter 26172 Ep: 1.78 loss 0.132 score 0.137 lr 8.8686e-06 
12/03/2021 06:12:14 - INFO - volta.train_utils -   [GQA]: iter 26252 Ep: 1.78 loss 0.133 score 0.138 lr 8.89575e-06 
12/03/2021 06:12:46 - INFO - volta.train_utils -   [GQA]: iter 26332 Ep: 1.79 loss 0.135 score 0.136 lr 8.9229e-06 
12/03/2021 06:13:19 - INFO - volta.train_utils -   [GQA]: iter 26412 Ep: 1.79 loss 0.136 score 0.134 lr 8.95005e-06 
12/03/2021 06:13:51 - INFO - volta.train_utils -   [GQA]: iter 26492 Ep: 1.80 loss 0.135 score 0.140 lr 8.9772e-06 
12/03/2021 06:14:24 - INFO - volta.train_utils -   [GQA]: iter 26572 Ep: 1.80 loss 0.128 score 0.141 lr 9.00434e-06 
12/03/2021 06:14:57 - INFO - volta.train_utils -   [GQA]: iter 26652 Ep: 1.81 loss 0.126 score 0.136 lr 9.03149e-06 
12/03/2021 06:15:30 - INFO - volta.train_utils -   [GQA]: iter 26732 Ep: 1.81 loss 0.125 score 0.136 lr 9.05864e-06 
12/03/2021 06:16:03 - INFO - volta.train_utils -   [GQA]: iter 26812 Ep: 1.82 loss 0.126 score 0.136 lr 9.08579e-06 
12/03/2021 06:16:35 - INFO - volta.train_utils -   [GQA]: iter 26892 Ep: 1.83 loss 0.133 score 0.139 lr 9.11294e-06 
12/03/2021 06:17:08 - INFO - volta.train_utils -   [GQA]: iter 26972 Ep: 1.83 loss 0.131 score 0.137 lr 9.14008e-06 
12/03/2021 06:17:41 - INFO - volta.train_utils -   [GQA]: iter 27052 Ep: 1.84 loss 0.126 score 0.136 lr 9.16723e-06 
12/03/2021 06:18:17 - INFO - volta.train_utils -   [GQA]: iter 27132 Ep: 1.84 loss 0.125 score 0.137 lr 9.19438e-06 
12/03/2021 06:18:54 - INFO - volta.train_utils -   [GQA]: iter 27212 Ep: 1.85 loss 0.138 score 0.134 lr 9.22153e-06 
12/03/2021 06:19:26 - INFO - volta.train_utils -   [GQA]: iter 27292 Ep: 1.85 loss 0.126 score 0.141 lr 9.24868e-06 
12/03/2021 06:19:59 - INFO - volta.train_utils -   [GQA]: iter 27372 Ep: 1.86 loss 0.128 score 0.136 lr 9.27582e-06 
12/03/2021 06:20:33 - INFO - volta.train_utils -   [GQA]: iter 27452 Ep: 1.86 loss 0.137 score 0.136 lr 9.30297e-06 
12/03/2021 06:21:06 - INFO - volta.train_utils -   [GQA]: iter 27532 Ep: 1.87 loss 0.124 score 0.136 lr 9.33012e-06 
12/03/2021 06:21:39 - INFO - volta.train_utils -   [GQA]: iter 27612 Ep: 1.87 loss 0.122 score 0.142 lr 9.35727e-06 
12/03/2021 06:22:12 - INFO - volta.train_utils -   [GQA]: iter 27692 Ep: 1.88 loss 0.130 score 0.134 lr 9.38442e-06 
12/03/2021 06:22:46 - INFO - volta.train_utils -   [GQA]: iter 27772 Ep: 1.88 loss 0.133 score 0.134 lr 9.41157e-06 
12/03/2021 06:23:18 - INFO - volta.train_utils -   [GQA]: iter 27852 Ep: 1.89 loss 0.129 score 0.140 lr 9.43871e-06 
12/03/2021 06:23:51 - INFO - volta.train_utils -   [GQA]: iter 27932 Ep: 1.90 loss 0.139 score 0.136 lr 9.46586e-06 
12/03/2021 06:24:24 - INFO - volta.train_utils -   [GQA]: iter 28012 Ep: 1.90 loss 0.129 score 0.141 lr 9.49301e-06 
12/03/2021 06:24:56 - INFO - volta.train_utils -   [GQA]: iter 28092 Ep: 1.91 loss 0.128 score 0.137 lr 9.52016e-06 
12/03/2021 06:25:30 - INFO - volta.train_utils -   [GQA]: iter 28172 Ep: 1.91 loss 0.126 score 0.140 lr 9.54731e-06 
12/03/2021 06:26:03 - INFO - volta.train_utils -   [GQA]: iter 28252 Ep: 1.92 loss 0.137 score 0.135 lr 9.57445e-06 
12/03/2021 06:26:35 - INFO - volta.train_utils -   [GQA]: iter 28332 Ep: 1.92 loss 0.130 score 0.133 lr 9.6016e-06 
12/03/2021 06:27:07 - INFO - volta.train_utils -   [GQA]: iter 28412 Ep: 1.93 loss 0.130 score 0.137 lr 9.62875e-06 
12/03/2021 06:27:41 - INFO - volta.train_utils -   [GQA]: iter 28492 Ep: 1.93 loss 0.128 score 0.142 lr 9.6559e-06 
12/03/2021 06:28:14 - INFO - volta.train_utils -   [GQA]: iter 28572 Ep: 1.94 loss 0.122 score 0.142 lr 9.68305e-06 
12/03/2021 06:28:46 - INFO - volta.train_utils -   [GQA]: iter 28652 Ep: 1.94 loss 0.136 score 0.136 lr 9.71019e-06 
12/03/2021 06:29:19 - INFO - volta.train_utils -   [GQA]: iter 28732 Ep: 1.95 loss 0.122 score 0.139 lr 9.73734e-06 
12/03/2021 06:29:51 - INFO - volta.train_utils -   [GQA]: iter 28812 Ep: 1.96 loss 0.133 score 0.136 lr 9.76449e-06 
12/03/2021 06:30:25 - INFO - volta.train_utils -   [GQA]: iter 28892 Ep: 1.96 loss 0.130 score 0.140 lr 9.79164e-06 
12/03/2021 06:30:58 - INFO - volta.train_utils -   [GQA]: iter 28972 Ep: 1.97 loss 0.124 score 0.140 lr 9.81879e-06 
12/03/2021 06:31:32 - INFO - volta.train_utils -   [GQA]: iter 29052 Ep: 1.97 loss 0.131 score 0.141 lr 9.84593e-06 
12/03/2021 06:32:09 - INFO - volta.train_utils -   [GQA]: iter 29132 Ep: 1.98 loss 0.130 score 0.136 lr 9.87308e-06 
12/03/2021 06:32:42 - INFO - volta.train_utils -   [GQA]: iter 29212 Ep: 1.98 loss 0.127 score 0.139 lr 9.90023e-06 
12/03/2021 06:33:15 - INFO - volta.train_utils -   [GQA]: iter 29292 Ep: 1.99 loss 0.130 score 0.138 lr 9.92738e-06 
12/03/2021 06:33:48 - INFO - volta.train_utils -   [GQA]: iter 29372 Ep: 1.99 loss 0.121 score 0.143 lr 9.95453e-06 
12/03/2021 06:34:21 - INFO - volta.train_utils -   [GQA]: iter 29452 Ep: 2.00 loss 0.137 score 0.137 lr 9.98168e-06 
12/03/2021 06:39:29 - INFO - volta.train_utils -   Eval task TASK15 on iteration 29464 
12/03/2021 06:39:29 - INFO - volta.train_utils -   Validation [GQA]: loss 2.069 score 54.815 
12/03/2021 06:39:29 - INFO - __main__ -   ** ** * Saving model * ** ** 
Epoch:  40%|████      | 2/5 [3:33:04<5:19:23, 6387.97s/it]12/03/2021 06:40:21 - INFO - volta.train_utils -   [GQA]: iter 29544 Ep: 2.01 loss 0.127 score 0.144 lr 9.9984e-06 
12/03/2021 06:40:54 - INFO - volta.train_utils -   [GQA]: iter 29624 Ep: 2.01 loss 0.125 score 0.142 lr 9.99555e-06 
12/03/2021 06:41:26 - INFO - volta.train_utils -   [GQA]: iter 29704 Ep: 2.02 loss 0.127 score 0.142 lr 9.99253e-06 
12/03/2021 06:41:59 - INFO - volta.train_utils -   [GQA]: iter 29784 Ep: 2.02 loss 0.128 score 0.139 lr 9.98952e-06 
12/03/2021 06:42:31 - INFO - volta.train_utils -   [GQA]: iter 29864 Ep: 2.03 loss 0.126 score 0.140 lr 9.9865e-06 
12/03/2021 06:43:04 - INFO - volta.train_utils -   [GQA]: iter 29944 Ep: 2.03 loss 0.129 score 0.141 lr 9.98348e-06 
12/03/2021 06:43:37 - INFO - volta.train_utils -   [GQA]: iter 30024 Ep: 2.04 loss 0.130 score 0.141 lr 9.98047e-06 
12/03/2021 06:44:10 - INFO - volta.train_utils -   [GQA]: iter 30104 Ep: 2.04 loss 0.126 score 0.145 lr 9.97745e-06 
12/03/2021 06:44:43 - INFO - volta.train_utils -   [GQA]: iter 30184 Ep: 2.05 loss 0.133 score 0.137 lr 9.97444e-06 
12/03/2021 06:45:15 - INFO - volta.train_utils -   [GQA]: iter 30264 Ep: 2.05 loss 0.128 score 0.134 lr 9.97142e-06 
12/03/2021 06:45:47 - INFO - volta.train_utils -   [GQA]: iter 30344 Ep: 2.06 loss 0.129 score 0.143 lr 9.9684e-06 
12/03/2021 06:46:20 - INFO - volta.train_utils -   [GQA]: iter 30424 Ep: 2.06 loss 0.129 score 0.138 lr 9.96539e-06 
12/03/2021 06:46:53 - INFO - volta.train_utils -   [GQA]: iter 30504 Ep: 2.07 loss 0.128 score 0.139 lr 9.96237e-06 
12/03/2021 06:47:25 - INFO - volta.train_utils -   [GQA]: iter 30584 Ep: 2.08 loss 0.123 score 0.139 lr 9.95935e-06 
12/03/2021 06:47:58 - INFO - volta.train_utils -   [GQA]: iter 30664 Ep: 2.08 loss 0.126 score 0.139 lr 9.95634e-06 
12/03/2021 06:48:31 - INFO - volta.train_utils -   [GQA]: iter 30744 Ep: 2.09 loss 0.130 score 0.137 lr 9.95332e-06 
12/03/2021 06:49:03 - INFO - volta.train_utils -   [GQA]: iter 30824 Ep: 2.09 loss 0.130 score 0.142 lr 9.9503e-06 
12/03/2021 06:49:36 - INFO - volta.train_utils -   [GQA]: iter 30904 Ep: 2.10 loss 0.114 score 0.144 lr 9.94729e-06 
12/03/2021 06:50:09 - INFO - volta.train_utils -   [GQA]: iter 30984 Ep: 2.10 loss 0.119 score 0.147 lr 9.94427e-06 
12/03/2021 06:50:42 - INFO - volta.train_utils -   [GQA]: iter 31064 Ep: 2.11 loss 0.121 score 0.142 lr 9.94125e-06 
12/03/2021 06:51:15 - INFO - volta.train_utils -   [GQA]: iter 31144 Ep: 2.11 loss 0.133 score 0.139 lr 9.93824e-06 
12/03/2021 06:51:47 - INFO - volta.train_utils -   [GQA]: iter 31224 Ep: 2.12 loss 0.130 score 0.138 lr 9.93522e-06 
12/03/2021 06:52:20 - INFO - volta.train_utils -   [GQA]: iter 31304 Ep: 2.12 loss 0.128 score 0.141 lr 9.93221e-06 
12/03/2021 06:52:52 - INFO - volta.train_utils -   [GQA]: iter 31384 Ep: 2.13 loss 0.115 score 0.143 lr 9.92919e-06 
12/03/2021 06:53:25 - INFO - volta.train_utils -   [GQA]: iter 31464 Ep: 2.14 loss 0.125 score 0.142 lr 9.92617e-06 
12/03/2021 06:54:06 - INFO - volta.train_utils -   [GQA]: iter 31544 Ep: 2.14 loss 0.122 score 0.142 lr 9.92316e-06 
12/03/2021 06:54:39 - INFO - volta.train_utils -   [GQA]: iter 31624 Ep: 2.15 loss 0.126 score 0.140 lr 9.92014e-06 
12/03/2021 06:55:12 - INFO - volta.train_utils -   [GQA]: iter 31704 Ep: 2.15 loss 0.120 score 0.142 lr 9.91712e-06 
12/03/2021 06:55:44 - INFO - volta.train_utils -   [GQA]: iter 31784 Ep: 2.16 loss 0.128 score 0.141 lr 9.91411e-06 
12/03/2021 06:56:17 - INFO - volta.train_utils -   [GQA]: iter 31864 Ep: 2.16 loss 0.121 score 0.142 lr 9.91109e-06 
12/03/2021 06:56:49 - INFO - volta.train_utils -   [GQA]: iter 31944 Ep: 2.17 loss 0.127 score 0.143 lr 9.90807e-06 
12/03/2021 06:57:21 - INFO - volta.train_utils -   [GQA]: iter 32024 Ep: 2.17 loss 0.126 score 0.145 lr 9.90506e-06 
12/03/2021 06:57:54 - INFO - volta.train_utils -   [GQA]: iter 32104 Ep: 2.18 loss 0.129 score 0.141 lr 9.90204e-06 
12/03/2021 06:58:27 - INFO - volta.train_utils -   [GQA]: iter 32184 Ep: 2.18 loss 0.122 score 0.144 lr 9.89902e-06 
12/03/2021 06:58:59 - INFO - volta.train_utils -   [GQA]: iter 32264 Ep: 2.19 loss 0.132 score 0.144 lr 9.89601e-06 
12/03/2021 06:59:31 - INFO - volta.train_utils -   [GQA]: iter 32344 Ep: 2.20 loss 0.129 score 0.141 lr 9.89299e-06 
12/03/2021 07:00:03 - INFO - volta.train_utils -   [GQA]: iter 32424 Ep: 2.20 loss 0.123 score 0.143 lr 9.88997e-06 
12/03/2021 07:00:36 - INFO - volta.train_utils -   [GQA]: iter 32504 Ep: 2.21 loss 0.120 score 0.142 lr 9.88696e-06 
12/03/2021 07:01:08 - INFO - volta.train_utils -   [GQA]: iter 32584 Ep: 2.21 loss 0.126 score 0.142 lr 9.88394e-06 
12/03/2021 07:01:41 - INFO - volta.train_utils -   [GQA]: iter 32664 Ep: 2.22 loss 0.131 score 0.142 lr 9.88093e-06 
12/03/2021 07:02:13 - INFO - volta.train_utils -   [GQA]: iter 32744 Ep: 2.22 loss 0.115 score 0.145 lr 9.87791e-06 
12/03/2021 07:02:46 - INFO - volta.train_utils -   [GQA]: iter 32824 Ep: 2.23 loss 0.117 score 0.147 lr 9.87489e-06 
12/03/2021 07:03:18 - INFO - volta.train_utils -   [GQA]: iter 32904 Ep: 2.23 loss 0.126 score 0.140 lr 9.87188e-06 
12/03/2021 07:03:51 - INFO - volta.train_utils -   [GQA]: iter 32984 Ep: 2.24 loss 0.129 score 0.143 lr 9.86886e-06 
12/03/2021 07:04:24 - INFO - volta.train_utils -   [GQA]: iter 33064 Ep: 2.24 loss 0.119 score 0.145 lr 9.86584e-06 
12/03/2021 07:04:57 - INFO - volta.train_utils -   [GQA]: iter 33144 Ep: 2.25 loss 0.121 score 0.145 lr 9.86283e-06 
12/03/2021 07:05:30 - INFO - volta.train_utils -   [GQA]: iter 33224 Ep: 2.25 loss 0.121 score 0.145 lr 9.85981e-06 
12/03/2021 07:06:02 - INFO - volta.train_utils -   [GQA]: iter 33304 Ep: 2.26 loss 0.125 score 0.142 lr 9.85679e-06 
12/03/2021 07:06:35 - INFO - volta.train_utils -   [GQA]: iter 33384 Ep: 2.27 loss 0.120 score 0.144 lr 9.85378e-06 
12/03/2021 07:07:13 - INFO - volta.train_utils -   [GQA]: iter 33464 Ep: 2.27 loss 0.119 score 0.142 lr 9.85076e-06 
12/03/2021 07:07:46 - INFO - volta.train_utils -   [GQA]: iter 33544 Ep: 2.28 loss 0.123 score 0.139 lr 9.84774e-06 
12/03/2021 07:08:19 - INFO - volta.train_utils -   [GQA]: iter 33624 Ep: 2.28 loss 0.122 score 0.143 lr 9.84473e-06 
12/03/2021 07:08:52 - INFO - volta.train_utils -   [GQA]: iter 33704 Ep: 2.29 loss 0.126 score 0.143 lr 9.84171e-06 
12/03/2021 07:09:24 - INFO - volta.train_utils -   [GQA]: iter 33784 Ep: 2.29 loss 0.121 score 0.145 lr 9.8387e-06 
12/03/2021 07:09:57 - INFO - volta.train_utils -   [GQA]: iter 33864 Ep: 2.30 loss 0.126 score 0.139 lr 9.83568e-06 
12/03/2021 07:10:29 - INFO - volta.train_utils -   [GQA]: iter 33944 Ep: 2.30 loss 0.117 score 0.146 lr 9.83266e-06 
12/03/2021 07:11:01 - INFO - volta.train_utils -   [GQA]: iter 34024 Ep: 2.31 loss 0.123 score 0.149 lr 9.82965e-06 
12/03/2021 07:11:34 - INFO - volta.train_utils -   [GQA]: iter 34104 Ep: 2.31 loss 0.116 score 0.148 lr 9.82663e-06 
12/03/2021 07:12:07 - INFO - volta.train_utils -   [GQA]: iter 34184 Ep: 2.32 loss 0.121 score 0.147 lr 9.82361e-06 
12/03/2021 07:12:40 - INFO - volta.train_utils -   [GQA]: iter 34264 Ep: 2.33 loss 0.127 score 0.141 lr 9.8206e-06 
12/03/2021 07:13:12 - INFO - volta.train_utils -   [GQA]: iter 34344 Ep: 2.33 loss 0.125 score 0.142 lr 9.81758e-06 
12/03/2021 07:13:45 - INFO - volta.train_utils -   [GQA]: iter 34424 Ep: 2.34 loss 0.127 score 0.143 lr 9.81456e-06 
12/03/2021 07:14:18 - INFO - volta.train_utils -   [GQA]: iter 34504 Ep: 2.34 loss 0.119 score 0.147 lr 9.81155e-06 
12/03/2021 07:14:51 - INFO - volta.train_utils -   [GQA]: iter 34584 Ep: 2.35 loss 0.123 score 0.144 lr 9.80853e-06 
12/03/2021 07:15:23 - INFO - volta.train_utils -   [GQA]: iter 34664 Ep: 2.35 loss 0.129 score 0.143 lr 9.80551e-06 
12/03/2021 07:15:56 - INFO - volta.train_utils -   [GQA]: iter 34744 Ep: 2.36 loss 0.122 score 0.147 lr 9.8025e-06 
12/03/2021 07:16:29 - INFO - volta.train_utils -   [GQA]: iter 34824 Ep: 2.36 loss 0.122 score 0.144 lr 9.79948e-06 
12/03/2021 07:17:01 - INFO - volta.train_utils -   [GQA]: iter 34904 Ep: 2.37 loss 0.126 score 0.145 lr 9.79646e-06 
12/03/2021 07:17:34 - INFO - volta.train_utils -   [GQA]: iter 34984 Ep: 2.37 loss 0.125 score 0.142 lr 9.79345e-06 
12/03/2021 07:18:06 - INFO - volta.train_utils -   [GQA]: iter 35064 Ep: 2.38 loss 0.118 score 0.144 lr 9.79043e-06 
12/03/2021 07:18:39 - INFO - volta.train_utils -   [GQA]: iter 35144 Ep: 2.39 loss 0.127 score 0.143 lr 9.78742e-06 
12/03/2021 07:19:12 - INFO - volta.train_utils -   [GQA]: iter 35224 Ep: 2.39 loss 0.120 score 0.142 lr 9.7844e-06 
12/03/2021 07:19:44 - INFO - volta.train_utils -   [GQA]: iter 35304 Ep: 2.40 loss 0.114 score 0.149 lr 9.78138e-06 
12/03/2021 07:20:22 - INFO - volta.train_utils -   [GQA]: iter 35384 Ep: 2.40 loss 0.117 score 0.150 lr 9.77837e-06 
12/03/2021 07:20:54 - INFO - volta.train_utils -   [GQA]: iter 35464 Ep: 2.41 loss 0.126 score 0.145 lr 9.77535e-06 
12/03/2021 07:21:25 - INFO - volta.train_utils -   [GQA]: iter 35544 Ep: 2.41 loss 0.113 score 0.146 lr 9.77233e-06 
12/03/2021 07:21:56 - INFO - volta.train_utils -   [GQA]: iter 35624 Ep: 2.42 loss 0.117 score 0.151 lr 9.76932e-06 
12/03/2021 07:22:27 - INFO - volta.train_utils -   [GQA]: iter 35704 Ep: 2.42 loss 0.118 score 0.147 lr 9.7663e-06 
12/03/2021 07:22:58 - INFO - volta.train_utils -   [GQA]: iter 35784 Ep: 2.43 loss 0.122 score 0.150 lr 9.76328e-06 
12/03/2021 07:23:29 - INFO - volta.train_utils -   [GQA]: iter 35864 Ep: 2.43 loss 0.120 score 0.145 lr 9.76027e-06 
12/03/2021 07:24:00 - INFO - volta.train_utils -   [GQA]: iter 35944 Ep: 2.44 loss 0.122 score 0.144 lr 9.75725e-06 
12/03/2021 07:24:32 - INFO - volta.train_utils -   [GQA]: iter 36024 Ep: 2.44 loss 0.119 score 0.147 lr 9.75423e-06 
12/03/2021 07:25:03 - INFO - volta.train_utils -   [GQA]: iter 36104 Ep: 2.45 loss 0.122 score 0.147 lr 9.75122e-06 
12/03/2021 07:25:34 - INFO - volta.train_utils -   [GQA]: iter 36184 Ep: 2.46 loss 0.123 score 0.148 lr 9.7482e-06 
12/03/2021 07:26:05 - INFO - volta.train_utils -   [GQA]: iter 36264 Ep: 2.46 loss 0.121 score 0.144 lr 9.74518e-06 
12/03/2021 07:26:37 - INFO - volta.train_utils -   [GQA]: iter 36344 Ep: 2.47 loss 0.117 score 0.145 lr 9.74217e-06 
12/03/2021 07:27:08 - INFO - volta.train_utils -   [GQA]: iter 36424 Ep: 2.47 loss 0.120 score 0.150 lr 9.73915e-06 
12/03/2021 07:27:39 - INFO - volta.train_utils -   [GQA]: iter 36504 Ep: 2.48 loss 0.121 score 0.148 lr 9.73614e-06 
12/03/2021 07:28:10 - INFO - volta.train_utils -   [GQA]: iter 36584 Ep: 2.48 loss 0.110 score 0.144 lr 9.73312e-06 
12/03/2021 07:28:41 - INFO - volta.train_utils -   [GQA]: iter 36664 Ep: 2.49 loss 0.118 score 0.152 lr 9.7301e-06 
12/03/2021 07:29:13 - INFO - volta.train_utils -   [GQA]: iter 36744 Ep: 2.49 loss 0.120 score 0.148 lr 9.72709e-06 
12/03/2021 07:29:44 - INFO - volta.train_utils -   [GQA]: iter 36824 Ep: 2.50 loss 0.120 score 0.148 lr 9.72407e-06 
12/03/2021 07:30:16 - INFO - volta.train_utils -   [GQA]: iter 36904 Ep: 2.50 loss 0.122 score 0.147 lr 9.72105e-06 
12/03/2021 07:30:47 - INFO - volta.train_utils -   [GQA]: iter 36984 Ep: 2.51 loss 0.121 score 0.144 lr 9.71804e-06 
12/03/2021 07:31:18 - INFO - volta.train_utils -   [GQA]: iter 37064 Ep: 2.52 loss 0.124 score 0.147 lr 9.71502e-06 
12/03/2021 07:31:50 - INFO - volta.train_utils -   [GQA]: iter 37144 Ep: 2.52 loss 0.126 score 0.146 lr 9.712e-06 
12/03/2021 07:32:21 - INFO - volta.train_utils -   [GQA]: iter 37224 Ep: 2.53 loss 0.125 score 0.146 lr 9.70899e-06 
12/03/2021 07:32:52 - INFO - volta.train_utils -   [GQA]: iter 37304 Ep: 2.53 loss 0.111 score 0.151 lr 9.70597e-06 
12/03/2021 07:33:24 - INFO - volta.train_utils -   [GQA]: iter 37384 Ep: 2.54 loss 0.114 score 0.151 lr 9.70295e-06 
12/03/2021 07:33:55 - INFO - volta.train_utils -   [GQA]: iter 37464 Ep: 2.54 loss 0.110 score 0.150 lr 9.69994e-06 
12/03/2021 07:34:26 - INFO - volta.train_utils -   [GQA]: iter 37544 Ep: 2.55 loss 0.116 score 0.149 lr 9.69692e-06 
12/03/2021 07:34:58 - INFO - volta.train_utils -   [GQA]: iter 37624 Ep: 2.55 loss 0.117 score 0.150 lr 9.69391e-06 
12/03/2021 07:35:29 - INFO - volta.train_utils -   [GQA]: iter 37704 Ep: 2.56 loss 0.113 score 0.155 lr 9.69089e-06 
12/03/2021 07:36:00 - INFO - volta.train_utils -   [GQA]: iter 37784 Ep: 2.56 loss 0.117 score 0.148 lr 9.68787e-06 
12/03/2021 07:36:31 - INFO - volta.train_utils -   [GQA]: iter 37864 Ep: 2.57 loss 0.116 score 0.148 lr 9.68486e-06 
12/03/2021 07:37:02 - INFO - volta.train_utils -   [GQA]: iter 37944 Ep: 2.58 loss 0.114 score 0.152 lr 9.68184e-06 
12/03/2021 07:37:33 - INFO - volta.train_utils -   [GQA]: iter 38024 Ep: 2.58 loss 0.113 score 0.149 lr 9.67882e-06 
12/03/2021 07:38:05 - INFO - volta.train_utils -   [GQA]: iter 38104 Ep: 2.59 loss 0.114 score 0.151 lr 9.67581e-06 
12/03/2021 07:38:36 - INFO - volta.train_utils -   [GQA]: iter 38184 Ep: 2.59 loss 0.120 score 0.152 lr 9.67279e-06 
12/03/2021 07:39:07 - INFO - volta.train_utils -   [GQA]: iter 38264 Ep: 2.60 loss 0.114 score 0.152 lr 9.66977e-06 
12/03/2021 07:39:38 - INFO - volta.train_utils -   [GQA]: iter 38344 Ep: 2.60 loss 0.127 score 0.148 lr 9.66676e-06 
12/03/2021 07:40:09 - INFO - volta.train_utils -   [GQA]: iter 38424 Ep: 2.61 loss 0.115 score 0.151 lr 9.66374e-06 
12/03/2021 07:40:41 - INFO - volta.train_utils -   [GQA]: iter 38504 Ep: 2.61 loss 0.114 score 0.152 lr 9.66072e-06 
12/03/2021 07:41:12 - INFO - volta.train_utils -   [GQA]: iter 38584 Ep: 2.62 loss 0.115 score 0.148 lr 9.65771e-06 
12/03/2021 07:41:43 - INFO - volta.train_utils -   [GQA]: iter 38664 Ep: 2.62 loss 0.114 score 0.152 lr 9.65469e-06 
12/03/2021 07:42:15 - INFO - volta.train_utils -   [GQA]: iter 38744 Ep: 2.63 loss 0.123 score 0.143 lr 9.65167e-06 
12/03/2021 07:42:46 - INFO - volta.train_utils -   [GQA]: iter 38824 Ep: 2.63 loss 0.122 score 0.150 lr 9.64866e-06 
12/03/2021 07:43:17 - INFO - volta.train_utils -   [GQA]: iter 38904 Ep: 2.64 loss 0.114 score 0.150 lr 9.64564e-06 
12/03/2021 07:43:48 - INFO - volta.train_utils -   [GQA]: iter 38984 Ep: 2.65 loss 0.111 score 0.154 lr 9.64263e-06 
12/03/2021 07:44:19 - INFO - volta.train_utils -   [GQA]: iter 39064 Ep: 2.65 loss 0.123 score 0.150 lr 9.63961e-06 
12/03/2021 07:44:51 - INFO - volta.train_utils -   [GQA]: iter 39144 Ep: 2.66 loss 0.115 score 0.154 lr 9.63659e-06 
12/03/2021 07:45:22 - INFO - volta.train_utils -   [GQA]: iter 39224 Ep: 2.66 loss 0.120 score 0.150 lr 9.63358e-06 
12/03/2021 07:45:54 - INFO - volta.train_utils -   [GQA]: iter 39304 Ep: 2.67 loss 0.112 score 0.152 lr 9.63056e-06 
12/03/2021 07:46:25 - INFO - volta.train_utils -   [GQA]: iter 39384 Ep: 2.67 loss 0.121 score 0.151 lr 9.62754e-06 
12/03/2021 07:46:56 - INFO - volta.train_utils -   [GQA]: iter 39464 Ep: 2.68 loss 0.114 score 0.152 lr 9.62453e-06 
12/03/2021 07:47:28 - INFO - volta.train_utils -   [GQA]: iter 39544 Ep: 2.68 loss 0.116 score 0.150 lr 9.62151e-06 
12/03/2021 07:47:59 - INFO - volta.train_utils -   [GQA]: iter 39624 Ep: 2.69 loss 0.120 score 0.151 lr 9.61849e-06 
12/03/2021 07:48:31 - INFO - volta.train_utils -   [GQA]: iter 39704 Ep: 2.69 loss 0.108 score 0.153 lr 9.61548e-06 
12/03/2021 07:49:02 - INFO - volta.train_utils -   [GQA]: iter 39784 Ep: 2.70 loss 0.116 score 0.150 lr 9.61246e-06 
12/03/2021 07:49:34 - INFO - volta.train_utils -   [GQA]: iter 39864 Ep: 2.71 loss 0.119 score 0.150 lr 9.60944e-06 
12/03/2021 07:50:06 - INFO - volta.train_utils -   [GQA]: iter 39944 Ep: 2.71 loss 0.109 score 0.156 lr 9.60643e-06 
12/03/2021 07:50:37 - INFO - volta.train_utils -   [GQA]: iter 40024 Ep: 2.72 loss 0.106 score 0.156 lr 9.60341e-06 
12/03/2021 07:51:09 - INFO - volta.train_utils -   [GQA]: iter 40104 Ep: 2.72 loss 0.114 score 0.155 lr 9.6004e-06 
12/03/2021 07:51:40 - INFO - volta.train_utils -   [GQA]: iter 40184 Ep: 2.73 loss 0.112 score 0.151 lr 9.59738e-06 
12/03/2021 07:52:11 - INFO - volta.train_utils -   [GQA]: iter 40264 Ep: 2.73 loss 0.115 score 0.154 lr 9.59436e-06 
12/03/2021 07:52:42 - INFO - volta.train_utils -   [GQA]: iter 40344 Ep: 2.74 loss 0.111 score 0.152 lr 9.59135e-06 
12/03/2021 07:53:14 - INFO - volta.train_utils -   [GQA]: iter 40424 Ep: 2.74 loss 0.105 score 0.155 lr 9.58833e-06 
12/03/2021 07:53:45 - INFO - volta.train_utils -   [GQA]: iter 40504 Ep: 2.75 loss 0.120 score 0.154 lr 9.58531e-06 
12/03/2021 07:54:16 - INFO - volta.train_utils -   [GQA]: iter 40584 Ep: 2.75 loss 0.112 score 0.153 lr 9.5823e-06 
12/03/2021 07:54:48 - INFO - volta.train_utils -   [GQA]: iter 40664 Ep: 2.76 loss 0.121 score 0.151 lr 9.57928e-06 
12/03/2021 07:55:19 - INFO - volta.train_utils -   [GQA]: iter 40744 Ep: 2.77 loss 0.109 score 0.154 lr 9.57626e-06 
12/03/2021 07:55:51 - INFO - volta.train_utils -   [GQA]: iter 40824 Ep: 2.77 loss 0.117 score 0.155 lr 9.57325e-06 
12/03/2021 07:56:22 - INFO - volta.train_utils -   [GQA]: iter 40904 Ep: 2.78 loss 0.114 score 0.156 lr 9.57023e-06 
12/03/2021 07:56:54 - INFO - volta.train_utils -   [GQA]: iter 40984 Ep: 2.78 loss 0.113 score 0.154 lr 9.56721e-06 
12/03/2021 07:57:26 - INFO - volta.train_utils -   [GQA]: iter 41064 Ep: 2.79 loss 0.114 score 0.154 lr 9.5642e-06 
12/03/2021 07:57:57 - INFO - volta.train_utils -   [GQA]: iter 41144 Ep: 2.79 loss 0.114 score 0.155 lr 9.56118e-06 
12/03/2021 07:58:29 - INFO - volta.train_utils -   [GQA]: iter 41224 Ep: 2.80 loss 0.118 score 0.152 lr 9.55816e-06 
12/03/2021 07:59:00 - INFO - volta.train_utils -   [GQA]: iter 41304 Ep: 2.80 loss 0.105 score 0.159 lr 9.55515e-06 
12/03/2021 07:59:31 - INFO - volta.train_utils -   [GQA]: iter 41384 Ep: 2.81 loss 0.115 score 0.153 lr 9.55213e-06 
12/03/2021 08:00:02 - INFO - volta.train_utils -   [GQA]: iter 41464 Ep: 2.81 loss 0.111 score 0.152 lr 9.54912e-06 
12/03/2021 08:00:34 - INFO - volta.train_utils -   [GQA]: iter 41544 Ep: 2.82 loss 0.112 score 0.154 lr 9.5461e-06 
12/03/2021 08:01:05 - INFO - volta.train_utils -   [GQA]: iter 41624 Ep: 2.83 loss 0.114 score 0.156 lr 9.54308e-06 
12/03/2021 08:01:36 - INFO - volta.train_utils -   [GQA]: iter 41704 Ep: 2.83 loss 0.112 score 0.149 lr 9.54007e-06 
12/03/2021 08:02:07 - INFO - volta.train_utils -   [GQA]: iter 41784 Ep: 2.84 loss 0.107 score 0.156 lr 9.53705e-06 
12/03/2021 08:02:39 - INFO - volta.train_utils -   [GQA]: iter 41864 Ep: 2.84 loss 0.116 score 0.153 lr 9.53403e-06 
12/03/2021 08:03:10 - INFO - volta.train_utils -   [GQA]: iter 41944 Ep: 2.85 loss 0.117 score 0.153 lr 9.53102e-06 
12/03/2021 08:03:42 - INFO - volta.train_utils -   [GQA]: iter 42024 Ep: 2.85 loss 0.103 score 0.156 lr 9.528e-06 
12/03/2021 08:04:13 - INFO - volta.train_utils -   [GQA]: iter 42104 Ep: 2.86 loss 0.108 score 0.155 lr 9.52498e-06 
12/03/2021 08:04:45 - INFO - volta.train_utils -   [GQA]: iter 42184 Ep: 2.86 loss 0.116 score 0.155 lr 9.52197e-06 
12/03/2021 08:05:16 - INFO - volta.train_utils -   [GQA]: iter 42264 Ep: 2.87 loss 0.113 score 0.155 lr 9.51895e-06 
12/03/2021 08:05:47 - INFO - volta.train_utils -   [GQA]: iter 42344 Ep: 2.87 loss 0.111 score 0.155 lr 9.51593e-06 
12/03/2021 08:06:19 - INFO - volta.train_utils -   [GQA]: iter 42424 Ep: 2.88 loss 0.111 score 0.154 lr 9.51292e-06 
12/03/2021 08:06:50 - INFO - volta.train_utils -   [GQA]: iter 42504 Ep: 2.88 loss 0.114 score 0.152 lr 9.5099e-06 
12/03/2021 08:07:21 - INFO - volta.train_utils -   [GQA]: iter 42584 Ep: 2.89 loss 0.111 score 0.156 lr 9.50689e-06 
12/03/2021 08:07:53 - INFO - volta.train_utils -   [GQA]: iter 42664 Ep: 2.90 loss 0.116 score 0.153 lr 9.50387e-06 
12/03/2021 08:08:24 - INFO - volta.train_utils -   [GQA]: iter 42744 Ep: 2.90 loss 0.102 score 0.159 lr 9.50085e-06 
12/03/2021 08:08:55 - INFO - volta.train_utils -   [GQA]: iter 42824 Ep: 2.91 loss 0.110 score 0.158 lr 9.49784e-06 
12/03/2021 08:09:27 - INFO - volta.train_utils -   [GQA]: iter 42904 Ep: 2.91 loss 0.112 score 0.157 lr 9.49482e-06 
12/03/2021 08:09:58 - INFO - volta.train_utils -   [GQA]: iter 42984 Ep: 2.92 loss 0.117 score 0.157 lr 9.4918e-06 
12/03/2021 08:10:30 - INFO - volta.train_utils -   [GQA]: iter 43064 Ep: 2.92 loss 0.109 score 0.150 lr 9.48879e-06 
12/03/2021 08:11:02 - INFO - volta.train_utils -   [GQA]: iter 43144 Ep: 2.93 loss 0.118 score 0.152 lr 9.48577e-06 
12/03/2021 08:11:33 - INFO - volta.train_utils -   [GQA]: iter 43224 Ep: 2.93 loss 0.105 score 0.159 lr 9.48275e-06 
12/03/2021 08:12:04 - INFO - volta.train_utils -   [GQA]: iter 43304 Ep: 2.94 loss 0.108 score 0.158 lr 9.47974e-06 
12/03/2021 08:12:35 - INFO - volta.train_utils -   [GQA]: iter 43384 Ep: 2.94 loss 0.110 score 0.155 lr 9.47672e-06 
12/03/2021 08:13:06 - INFO - volta.train_utils -   [GQA]: iter 43464 Ep: 2.95 loss 0.104 score 0.157 lr 9.4737e-06 
12/03/2021 08:13:37 - INFO - volta.train_utils -   [GQA]: iter 43544 Ep: 2.96 loss 0.112 score 0.156 lr 9.47069e-06 
12/03/2021 08:14:09 - INFO - volta.train_utils -   [GQA]: iter 43624 Ep: 2.96 loss 0.110 score 0.157 lr 9.46767e-06 
12/03/2021 08:14:40 - INFO - volta.train_utils -   [GQA]: iter 43704 Ep: 2.97 loss 0.109 score 0.156 lr 9.46465e-06 
12/03/2021 08:15:12 - INFO - volta.train_utils -   [GQA]: iter 43784 Ep: 2.97 loss 0.106 score 0.159 lr 9.46164e-06 
12/03/2021 08:15:43 - INFO - volta.train_utils -   [GQA]: iter 43864 Ep: 2.98 loss 0.112 score 0.155 lr 9.45862e-06 
12/03/2021 08:16:15 - INFO - volta.train_utils -   [GQA]: iter 43944 Ep: 2.98 loss 0.106 score 0.157 lr 9.45561e-06 
12/03/2021 08:16:46 - INFO - volta.train_utils -   [GQA]: iter 44024 Ep: 2.99 loss 0.113 score 0.155 lr 9.45259e-06 
12/03/2021 08:17:18 - INFO - volta.train_utils -   [GQA]: iter 44104 Ep: 2.99 loss 0.101 score 0.160 lr 9.44957e-06 
12/03/2021 08:17:49 - INFO - volta.train_utils -   [GQA]: iter 44184 Ep: 3.00 loss 0.115 score 0.155 lr 9.44656e-06 
12/03/2021 08:22:53 - INFO - volta.train_utils -   Eval task TASK15 on iteration 44196 
12/03/2021 08:22:53 - INFO - volta.train_utils -   Validation [GQA]: loss 1.926 score 59.295 
12/03/2021 08:22:53 - INFO - __main__ -   ** ** * Saving model * ** ** 
Epoch:  60%|██████    | 3/5 [5:16:25<3:31:03, 6331.85s/it]12/03/2021 08:23:42 - INFO - volta.train_utils -   [GQA]: iter 44276 Ep: 3.01 loss 0.106 score 0.162 lr 9.44331e-06 
12/03/2021 08:24:13 - INFO - volta.train_utils -   [GQA]: iter 44356 Ep: 3.01 loss 0.111 score 0.158 lr 9.44007e-06 
12/03/2021 08:24:45 - INFO - volta.train_utils -   [GQA]: iter 44436 Ep: 3.02 loss 0.106 score 0.158 lr 9.43705e-06 
12/03/2021 08:25:16 - INFO - volta.train_utils -   [GQA]: iter 44516 Ep: 3.02 loss 0.107 score 0.158 lr 9.43404e-06 
12/03/2021 08:25:47 - INFO - volta.train_utils -   [GQA]: iter 44596 Ep: 3.03 loss 0.104 score 0.161 lr 9.43102e-06 
12/03/2021 08:26:19 - INFO - volta.train_utils -   [GQA]: iter 44676 Ep: 3.03 loss 0.102 score 0.159 lr 9.428e-06 
12/03/2021 08:26:50 - INFO - volta.train_utils -   [GQA]: iter 44756 Ep: 3.04 loss 0.108 score 0.158 lr 9.42499e-06 
12/03/2021 08:27:21 - INFO - volta.train_utils -   [GQA]: iter 44836 Ep: 3.04 loss 0.104 score 0.161 lr 9.42197e-06 
12/03/2021 08:27:52 - INFO - volta.train_utils -   [GQA]: iter 44916 Ep: 3.05 loss 0.108 score 0.157 lr 9.41896e-06 
12/03/2021 08:28:24 - INFO - volta.train_utils -   [GQA]: iter 44996 Ep: 3.05 loss 0.108 score 0.153 lr 9.41594e-06 
12/03/2021 08:28:55 - INFO - volta.train_utils -   [GQA]: iter 45076 Ep: 3.06 loss 0.107 score 0.160 lr 9.41292e-06 
12/03/2021 08:29:27 - INFO - volta.train_utils -   [GQA]: iter 45156 Ep: 3.06 loss 0.109 score 0.156 lr 9.40991e-06 
12/03/2021 08:29:58 - INFO - volta.train_utils -   [GQA]: iter 45236 Ep: 3.07 loss 0.110 score 0.159 lr 9.40689e-06 
12/03/2021 08:30:29 - INFO - volta.train_utils -   [GQA]: iter 45316 Ep: 3.08 loss 0.110 score 0.156 lr 9.40387e-06 
12/03/2021 08:31:00 - INFO - volta.train_utils -   [GQA]: iter 45396 Ep: 3.08 loss 0.107 score 0.157 lr 9.40086e-06 
12/03/2021 08:31:32 - INFO - volta.train_utils -   [GQA]: iter 45476 Ep: 3.09 loss 0.109 score 0.155 lr 9.39784e-06 
12/03/2021 08:32:03 - INFO - volta.train_utils -   [GQA]: iter 45556 Ep: 3.09 loss 0.104 score 0.161 lr 9.39482e-06 
12/03/2021 08:32:35 - INFO - volta.train_utils -   [GQA]: iter 45636 Ep: 3.10 loss 0.103 score 0.162 lr 9.39181e-06 
12/03/2021 08:33:06 - INFO - volta.train_utils -   [GQA]: iter 45716 Ep: 3.10 loss 0.101 score 0.164 lr 9.38879e-06 
12/03/2021 08:33:38 - INFO - volta.train_utils -   [GQA]: iter 45796 Ep: 3.11 loss 0.101 score 0.164 lr 9.38577e-06 
12/03/2021 08:34:09 - INFO - volta.train_utils -   [GQA]: iter 45876 Ep: 3.11 loss 0.112 score 0.159 lr 9.38276e-06 
12/03/2021 08:34:40 - INFO - volta.train_utils -   [GQA]: iter 45956 Ep: 3.12 loss 0.106 score 0.158 lr 9.37974e-06 
12/03/2021 08:35:11 - INFO - volta.train_utils -   [GQA]: iter 46036 Ep: 3.12 loss 0.107 score 0.159 lr 9.37673e-06 
12/03/2021 08:35:43 - INFO - volta.train_utils -   [GQA]: iter 46116 Ep: 3.13 loss 0.104 score 0.162 lr 9.37371e-06 
12/03/2021 08:36:14 - INFO - volta.train_utils -   [GQA]: iter 46196 Ep: 3.14 loss 0.103 score 0.161 lr 9.37069e-06 
12/03/2021 08:36:46 - INFO - volta.train_utils -   [GQA]: iter 46276 Ep: 3.14 loss 0.108 score 0.160 lr 9.36768e-06 
12/03/2021 08:37:17 - INFO - volta.train_utils -   [GQA]: iter 46356 Ep: 3.15 loss 0.112 score 0.158 lr 9.36466e-06 
12/03/2021 08:37:48 - INFO - volta.train_utils -   [GQA]: iter 46436 Ep: 3.15 loss 0.097 score 0.160 lr 9.36164e-06 
12/03/2021 08:38:20 - INFO - volta.train_utils -   [GQA]: iter 46516 Ep: 3.16 loss 0.106 score 0.157 lr 9.35863e-06 
12/03/2021 08:38:51 - INFO - volta.train_utils -   [GQA]: iter 46596 Ep: 3.16 loss 0.107 score 0.158 lr 9.35561e-06 
12/03/2021 08:39:22 - INFO - volta.train_utils -   [GQA]: iter 46676 Ep: 3.17 loss 0.107 score 0.161 lr 9.35259e-06 
12/03/2021 08:39:54 - INFO - volta.train_utils -   [GQA]: iter 46756 Ep: 3.17 loss 0.104 score 0.161 lr 9.34958e-06 
12/03/2021 08:40:25 - INFO - volta.train_utils -   [GQA]: iter 46836 Ep: 3.18 loss 0.107 score 0.160 lr 9.34656e-06 
12/03/2021 08:40:57 - INFO - volta.train_utils -   [GQA]: iter 46916 Ep: 3.18 loss 0.100 score 0.162 lr 9.34354e-06 
12/03/2021 08:41:28 - INFO - volta.train_utils -   [GQA]: iter 46996 Ep: 3.19 loss 0.112 score 0.162 lr 9.34053e-06 
12/03/2021 08:41:59 - INFO - volta.train_utils -   [GQA]: iter 47076 Ep: 3.20 loss 0.106 score 0.158 lr 9.33751e-06 
12/03/2021 08:42:31 - INFO - volta.train_utils -   [GQA]: iter 47156 Ep: 3.20 loss 0.107 score 0.162 lr 9.33449e-06 
12/03/2021 08:43:02 - INFO - volta.train_utils -   [GQA]: iter 47236 Ep: 3.21 loss 0.100 score 0.163 lr 9.33148e-06 
12/03/2021 08:43:34 - INFO - volta.train_utils -   [GQA]: iter 47316 Ep: 3.21 loss 0.106 score 0.160 lr 9.32846e-06 
12/03/2021 08:44:05 - INFO - volta.train_utils -   [GQA]: iter 47396 Ep: 3.22 loss 0.109 score 0.160 lr 9.32545e-06 
12/03/2021 08:44:37 - INFO - volta.train_utils -   [GQA]: iter 47476 Ep: 3.22 loss 0.099 score 0.164 lr 9.32243e-06 
12/03/2021 08:45:08 - INFO - volta.train_utils -   [GQA]: iter 47556 Ep: 3.23 loss 0.102 score 0.166 lr 9.31941e-06 
12/03/2021 08:45:40 - INFO - volta.train_utils -   [GQA]: iter 47636 Ep: 3.23 loss 0.108 score 0.160 lr 9.3164e-06 
12/03/2021 08:46:11 - INFO - volta.train_utils -   [GQA]: iter 47716 Ep: 3.24 loss 0.113 score 0.154 lr 9.31338e-06 
12/03/2021 08:46:43 - INFO - volta.train_utils -   [GQA]: iter 47796 Ep: 3.24 loss 0.097 score 0.162 lr 9.31036e-06 
12/03/2021 08:47:14 - INFO - volta.train_utils -   [GQA]: iter 47876 Ep: 3.25 loss 0.106 score 0.163 lr 9.30735e-06 
12/03/2021 08:47:45 - INFO - volta.train_utils -   [GQA]: iter 47956 Ep: 3.25 loss 0.101 score 0.160 lr 9.30433e-06 
12/03/2021 08:48:17 - INFO - volta.train_utils -   [GQA]: iter 48036 Ep: 3.26 loss 0.105 score 0.159 lr 9.30131e-06 
12/03/2021 08:48:48 - INFO - volta.train_utils -   [GQA]: iter 48116 Ep: 3.27 loss 0.100 score 0.162 lr 9.2983e-06 
12/03/2021 08:49:19 - INFO - volta.train_utils -   [GQA]: iter 48196 Ep: 3.27 loss 0.107 score 0.160 lr 9.29528e-06 
12/03/2021 08:49:51 - INFO - volta.train_utils -   [GQA]: iter 48276 Ep: 3.28 loss 0.105 score 0.160 lr 9.29226e-06 
12/03/2021 08:50:22 - INFO - volta.train_utils -   [GQA]: iter 48356 Ep: 3.28 loss 0.105 score 0.161 lr 9.28925e-06 
12/03/2021 08:50:53 - INFO - volta.train_utils -   [GQA]: iter 48436 Ep: 3.29 loss 0.103 score 0.161 lr 9.28623e-06 
12/03/2021 08:51:25 - INFO - volta.train_utils -   [GQA]: iter 48516 Ep: 3.29 loss 0.108 score 0.160 lr 9.28321e-06 
12/03/2021 08:51:56 - INFO - volta.train_utils -   [GQA]: iter 48596 Ep: 3.30 loss 0.106 score 0.156 lr 9.2802e-06 
12/03/2021 08:52:27 - INFO - volta.train_utils -   [GQA]: iter 48676 Ep: 3.30 loss 0.097 score 0.163 lr 9.27718e-06 
12/03/2021 08:52:59 - INFO - volta.train_utils -   [GQA]: iter 48756 Ep: 3.31 loss 0.097 score 0.168 lr 9.27417e-06 
12/03/2021 08:53:30 - INFO - volta.train_utils -   [GQA]: iter 48836 Ep: 3.31 loss 0.103 score 0.163 lr 9.27115e-06 
12/03/2021 08:54:02 - INFO - volta.train_utils -   [GQA]: iter 48916 Ep: 3.32 loss 0.099 score 0.164 lr 9.26813e-06 
12/03/2021 08:54:34 - INFO - volta.train_utils -   [GQA]: iter 48996 Ep: 3.33 loss 0.105 score 0.159 lr 9.26512e-06 
12/03/2021 08:55:05 - INFO - volta.train_utils -   [GQA]: iter 49076 Ep: 3.33 loss 0.109 score 0.158 lr 9.2621e-06 
12/03/2021 08:55:37 - INFO - volta.train_utils -   [GQA]: iter 49156 Ep: 3.34 loss 0.106 score 0.160 lr 9.25908e-06 
12/03/2021 08:56:08 - INFO - volta.train_utils -   [GQA]: iter 49236 Ep: 3.34 loss 0.096 score 0.165 lr 9.25607e-06 
12/03/2021 08:56:40 - INFO - volta.train_utils -   [GQA]: iter 49316 Ep: 3.35 loss 0.103 score 0.161 lr 9.25305e-06 
12/03/2021 08:57:12 - INFO - volta.train_utils -   [GQA]: iter 49396 Ep: 3.35 loss 0.103 score 0.162 lr 9.25003e-06 
12/03/2021 08:57:43 - INFO - volta.train_utils -   [GQA]: iter 49476 Ep: 3.36 loss 0.104 score 0.165 lr 9.24702e-06 
12/03/2021 08:58:15 - INFO - volta.train_utils -   [GQA]: iter 49556 Ep: 3.36 loss 0.104 score 0.160 lr 9.244e-06 
12/03/2021 08:58:46 - INFO - volta.train_utils -   [GQA]: iter 49636 Ep: 3.37 loss 0.104 score 0.163 lr 9.24098e-06 
12/03/2021 08:59:18 - INFO - volta.train_utils -   [GQA]: iter 49716 Ep: 3.37 loss 0.104 score 0.159 lr 9.23797e-06 
12/03/2021 08:59:50 - INFO - volta.train_utils -   [GQA]: iter 49796 Ep: 3.38 loss 0.102 score 0.159 lr 9.23495e-06 
12/03/2021 09:00:21 - INFO - volta.train_utils -   [GQA]: iter 49876 Ep: 3.39 loss 0.106 score 0.163 lr 9.23194e-06 
12/03/2021 09:00:53 - INFO - volta.train_utils -   [GQA]: iter 49956 Ep: 3.39 loss 0.096 score 0.162 lr 9.22892e-06 
12/03/2021 09:01:24 - INFO - volta.train_utils -   [GQA]: iter 50036 Ep: 3.40 loss 0.103 score 0.162 lr 9.2259e-06 
12/03/2021 09:01:55 - INFO - volta.train_utils -   [GQA]: iter 50116 Ep: 3.40 loss 0.097 score 0.165 lr 9.22289e-06 
12/03/2021 09:02:26 - INFO - volta.train_utils -   [GQA]: iter 50196 Ep: 3.41 loss 0.109 score 0.162 lr 9.21987e-06 
12/03/2021 09:02:57 - INFO - volta.train_utils -   [GQA]: iter 50276 Ep: 3.41 loss 0.099 score 0.164 lr 9.21685e-06 
12/03/2021 09:03:28 - INFO - volta.train_utils -   [GQA]: iter 50356 Ep: 3.42 loss 0.102 score 0.163 lr 9.21384e-06 
12/03/2021 09:03:59 - INFO - volta.train_utils -   [GQA]: iter 50436 Ep: 3.42 loss 0.097 score 0.163 lr 9.21082e-06 
12/03/2021 09:04:30 - INFO - volta.train_utils -   [GQA]: iter 50516 Ep: 3.43 loss 0.100 score 0.167 lr 9.2078e-06 
12/03/2021 09:05:01 - INFO - volta.train_utils -   [GQA]: iter 50596 Ep: 3.43 loss 0.103 score 0.159 lr 9.20479e-06 
12/03/2021 09:05:32 - INFO - volta.train_utils -   [GQA]: iter 50676 Ep: 3.44 loss 0.102 score 0.160 lr 9.20177e-06 
12/03/2021 09:06:03 - INFO - volta.train_utils -   [GQA]: iter 50756 Ep: 3.44 loss 0.099 score 0.163 lr 9.19875e-06 
12/03/2021 09:06:34 - INFO - volta.train_utils -   [GQA]: iter 50836 Ep: 3.45 loss 0.097 score 0.162 lr 9.19574e-06 
12/03/2021 09:07:06 - INFO - volta.train_utils -   [GQA]: iter 50916 Ep: 3.46 loss 0.098 score 0.161 lr 9.19272e-06 
12/03/2021 09:07:37 - INFO - volta.train_utils -   [GQA]: iter 50996 Ep: 3.46 loss 0.103 score 0.162 lr 9.1897e-06 
12/03/2021 09:08:08 - INFO - volta.train_utils -   [GQA]: iter 51076 Ep: 3.47 loss 0.100 score 0.162 lr 9.18669e-06 
12/03/2021 09:08:39 - INFO - volta.train_utils -   [GQA]: iter 51156 Ep: 3.47 loss 0.102 score 0.164 lr 9.18367e-06 
12/03/2021 09:09:10 - INFO - volta.train_utils -   [GQA]: iter 51236 Ep: 3.48 loss 0.105 score 0.161 lr 9.18066e-06 
12/03/2021 09:09:41 - INFO - volta.train_utils -   [GQA]: iter 51316 Ep: 3.48 loss 0.101 score 0.163 lr 9.17764e-06 
12/03/2021 09:10:12 - INFO - volta.train_utils -   [GQA]: iter 51396 Ep: 3.49 loss 0.099 score 0.167 lr 9.17462e-06 
12/03/2021 09:10:43 - INFO - volta.train_utils -   [GQA]: iter 51476 Ep: 3.49 loss 0.099 score 0.161 lr 9.17161e-06 
12/03/2021 09:11:14 - INFO - volta.train_utils -   [GQA]: iter 51556 Ep: 3.50 loss 0.101 score 0.163 lr 9.16859e-06 
12/03/2021 09:11:45 - INFO - volta.train_utils -   [GQA]: iter 51636 Ep: 3.50 loss 0.103 score 0.164 lr 9.16557e-06 
12/03/2021 09:12:16 - INFO - volta.train_utils -   [GQA]: iter 51716 Ep: 3.51 loss 0.102 score 0.162 lr 9.16256e-06 
12/03/2021 09:12:47 - INFO - volta.train_utils -   [GQA]: iter 51796 Ep: 3.52 loss 0.103 score 0.164 lr 9.15954e-06 
12/03/2021 09:13:18 - INFO - volta.train_utils -   [GQA]: iter 51876 Ep: 3.52 loss 0.106 score 0.162 lr 9.15652e-06 
12/03/2021 09:13:50 - INFO - volta.train_utils -   [GQA]: iter 51956 Ep: 3.53 loss 0.101 score 0.163 lr 9.15351e-06 
12/03/2021 09:14:21 - INFO - volta.train_utils -   [GQA]: iter 52036 Ep: 3.53 loss 0.095 score 0.167 lr 9.15049e-06 
12/03/2021 09:14:52 - INFO - volta.train_utils -   [GQA]: iter 52116 Ep: 3.54 loss 0.097 score 0.169 lr 9.14747e-06 
12/03/2021 09:15:23 - INFO - volta.train_utils -   [GQA]: iter 52196 Ep: 3.54 loss 0.101 score 0.165 lr 9.14446e-06 
12/03/2021 09:15:54 - INFO - volta.train_utils -   [GQA]: iter 52276 Ep: 3.55 loss 0.100 score 0.164 lr 9.14144e-06 
12/03/2021 09:16:25 - INFO - volta.train_utils -   [GQA]: iter 52356 Ep: 3.55 loss 0.104 score 0.165 lr 9.13843e-06 
12/03/2021 09:16:56 - INFO - volta.train_utils -   [GQA]: iter 52436 Ep: 3.56 loss 0.098 score 0.171 lr 9.13541e-06 
12/03/2021 09:17:27 - INFO - volta.train_utils -   [GQA]: iter 52516 Ep: 3.56 loss 0.098 score 0.166 lr 9.13239e-06 
12/03/2021 09:17:58 - INFO - volta.train_utils -   [GQA]: iter 52596 Ep: 3.57 loss 0.107 score 0.165 lr 9.12938e-06 
12/03/2021 09:18:30 - INFO - volta.train_utils -   [GQA]: iter 52676 Ep: 3.58 loss 0.096 score 0.168 lr 9.12636e-06 
12/03/2021 09:19:01 - INFO - volta.train_utils -   [GQA]: iter 52756 Ep: 3.58 loss 0.100 score 0.165 lr 9.12334e-06 
12/03/2021 09:19:32 - INFO - volta.train_utils -   [GQA]: iter 52836 Ep: 3.59 loss 0.097 score 0.169 lr 9.12033e-06 
12/03/2021 09:20:04 - INFO - volta.train_utils -   [GQA]: iter 52916 Ep: 3.59 loss 0.102 score 0.167 lr 9.11731e-06 
12/03/2021 09:20:35 - INFO - volta.train_utils -   [GQA]: iter 52996 Ep: 3.60 loss 0.094 score 0.168 lr 9.11429e-06 
12/03/2021 09:21:06 - INFO - volta.train_utils -   [GQA]: iter 53076 Ep: 3.60 loss 0.102 score 0.165 lr 9.11128e-06 
12/03/2021 09:21:37 - INFO - volta.train_utils -   [GQA]: iter 53156 Ep: 3.61 loss 0.099 score 0.164 lr 9.10826e-06 
12/03/2021 09:22:08 - INFO - volta.train_utils -   [GQA]: iter 53236 Ep: 3.61 loss 0.097 score 0.167 lr 9.10524e-06 
12/03/2021 09:22:40 - INFO - volta.train_utils -   [GQA]: iter 53316 Ep: 3.62 loss 0.102 score 0.162 lr 9.10223e-06 
12/03/2021 09:23:10 - INFO - volta.train_utils -   [GQA]: iter 53396 Ep: 3.62 loss 0.105 score 0.167 lr 9.09921e-06 
12/03/2021 09:23:42 - INFO - volta.train_utils -   [GQA]: iter 53476 Ep: 3.63 loss 0.104 score 0.161 lr 9.09619e-06 
12/03/2021 09:24:13 - INFO - volta.train_utils -   [GQA]: iter 53556 Ep: 3.63 loss 0.097 score 0.164 lr 9.09318e-06 
12/03/2021 09:24:44 - INFO - volta.train_utils -   [GQA]: iter 53636 Ep: 3.64 loss 0.101 score 0.167 lr 9.09016e-06 
12/03/2021 09:25:16 - INFO - volta.train_utils -   [GQA]: iter 53716 Ep: 3.65 loss 0.090 score 0.170 lr 9.08715e-06 
12/03/2021 09:25:47 - INFO - volta.train_utils -   [GQA]: iter 53796 Ep: 3.65 loss 0.099 score 0.165 lr 9.08413e-06 
12/03/2021 09:26:18 - INFO - volta.train_utils -   [GQA]: iter 53876 Ep: 3.66 loss 0.103 score 0.167 lr 9.08111e-06 
12/03/2021 09:26:49 - INFO - volta.train_utils -   [GQA]: iter 53956 Ep: 3.66 loss 0.098 score 0.168 lr 9.0781e-06 
12/03/2021 09:27:21 - INFO - volta.train_utils -   [GQA]: iter 54036 Ep: 3.67 loss 0.101 score 0.166 lr 9.07508e-06 
12/03/2021 09:27:52 - INFO - volta.train_utils -   [GQA]: iter 54116 Ep: 3.67 loss 0.099 score 0.166 lr 9.07206e-06 
12/03/2021 09:28:23 - INFO - volta.train_utils -   [GQA]: iter 54196 Ep: 3.68 loss 0.100 score 0.167 lr 9.06905e-06 
12/03/2021 09:28:54 - INFO - volta.train_utils -   [GQA]: iter 54276 Ep: 3.68 loss 0.102 score 0.164 lr 9.06603e-06 
12/03/2021 09:29:25 - INFO - volta.train_utils -   [GQA]: iter 54356 Ep: 3.69 loss 0.095 score 0.167 lr 9.06301e-06 
12/03/2021 09:29:56 - INFO - volta.train_utils -   [GQA]: iter 54436 Ep: 3.69 loss 0.095 score 0.168 lr 9.06e-06 
12/03/2021 09:30:28 - INFO - volta.train_utils -   [GQA]: iter 54516 Ep: 3.70 loss 0.097 score 0.169 lr 9.05698e-06 
12/03/2021 09:30:59 - INFO - volta.train_utils -   [GQA]: iter 54596 Ep: 3.71 loss 0.101 score 0.166 lr 9.05396e-06 
12/03/2021 09:31:30 - INFO - volta.train_utils -   [GQA]: iter 54676 Ep: 3.71 loss 0.098 score 0.171 lr 9.05095e-06 
12/03/2021 09:32:01 - INFO - volta.train_utils -   [GQA]: iter 54756 Ep: 3.72 loss 0.094 score 0.171 lr 9.04793e-06 
12/03/2021 09:32:32 - INFO - volta.train_utils -   [GQA]: iter 54836 Ep: 3.72 loss 0.094 score 0.172 lr 9.04492e-06 
12/03/2021 09:33:03 - INFO - volta.train_utils -   [GQA]: iter 54916 Ep: 3.73 loss 0.098 score 0.169 lr 9.0419e-06 
12/03/2021 09:33:34 - INFO - volta.train_utils -   [GQA]: iter 54996 Ep: 3.73 loss 0.098 score 0.169 lr 9.03888e-06 
12/03/2021 09:34:05 - INFO - volta.train_utils -   [GQA]: iter 55076 Ep: 3.74 loss 0.099 score 0.168 lr 9.03587e-06 
12/03/2021 09:34:36 - INFO - volta.train_utils -   [GQA]: iter 55156 Ep: 3.74 loss 0.096 score 0.172 lr 9.03285e-06 
12/03/2021 09:35:08 - INFO - volta.train_utils -   [GQA]: iter 55236 Ep: 3.75 loss 0.093 score 0.171 lr 9.02983e-06 
12/03/2021 09:35:39 - INFO - volta.train_utils -   [GQA]: iter 55316 Ep: 3.75 loss 0.100 score 0.169 lr 9.02682e-06 
12/03/2021 09:36:10 - INFO - volta.train_utils -   [GQA]: iter 55396 Ep: 3.76 loss 0.092 score 0.170 lr 9.0238e-06 
12/03/2021 09:36:41 - INFO - volta.train_utils -   [GQA]: iter 55476 Ep: 3.77 loss 0.095 score 0.170 lr 9.02078e-06 
12/03/2021 09:37:12 - INFO - volta.train_utils -   [GQA]: iter 55556 Ep: 3.77 loss 0.101 score 0.170 lr 9.01777e-06 
12/03/2021 09:37:43 - INFO - volta.train_utils -   [GQA]: iter 55636 Ep: 3.78 loss 0.088 score 0.172 lr 9.01475e-06 
12/03/2021 09:38:14 - INFO - volta.train_utils -   [GQA]: iter 55716 Ep: 3.78 loss 0.095 score 0.171 lr 9.01173e-06 
12/03/2021 09:38:45 - INFO - volta.train_utils -   [GQA]: iter 55796 Ep: 3.79 loss 0.094 score 0.171 lr 9.00872e-06 
12/03/2021 09:39:16 - INFO - volta.train_utils -   [GQA]: iter 55876 Ep: 3.79 loss 0.091 score 0.171 lr 9.0057e-06 
12/03/2021 09:39:48 - INFO - volta.train_utils -   [GQA]: iter 55956 Ep: 3.80 loss 0.106 score 0.170 lr 9.00268e-06 
12/03/2021 09:40:19 - INFO - volta.train_utils -   [GQA]: iter 56036 Ep: 3.80 loss 0.090 score 0.173 lr 8.99967e-06 
12/03/2021 09:40:50 - INFO - volta.train_utils -   [GQA]: iter 56116 Ep: 3.81 loss 0.101 score 0.168 lr 8.99665e-06 
12/03/2021 09:41:21 - INFO - volta.train_utils -   [GQA]: iter 56196 Ep: 3.81 loss 0.095 score 0.167 lr 8.99364e-06 
12/03/2021 09:41:52 - INFO - volta.train_utils -   [GQA]: iter 56276 Ep: 3.82 loss 0.095 score 0.170 lr 8.99062e-06 
12/03/2021 09:42:24 - INFO - volta.train_utils -   [GQA]: iter 56356 Ep: 3.82 loss 0.093 score 0.172 lr 8.9876e-06 
12/03/2021 09:42:55 - INFO - volta.train_utils -   [GQA]: iter 56436 Ep: 3.83 loss 0.097 score 0.169 lr 8.98459e-06 
12/03/2021 09:43:26 - INFO - volta.train_utils -   [GQA]: iter 56516 Ep: 3.84 loss 0.092 score 0.173 lr 8.98157e-06 
12/03/2021 09:43:57 - INFO - volta.train_utils -   [GQA]: iter 56596 Ep: 3.84 loss 0.099 score 0.168 lr 8.97855e-06 
12/03/2021 09:44:29 - INFO - volta.train_utils -   [GQA]: iter 56676 Ep: 3.85 loss 0.099 score 0.168 lr 8.97554e-06 
12/03/2021 09:45:00 - INFO - volta.train_utils -   [GQA]: iter 56756 Ep: 3.85 loss 0.094 score 0.175 lr 8.97252e-06 
12/03/2021 09:45:31 - INFO - volta.train_utils -   [GQA]: iter 56836 Ep: 3.86 loss 0.097 score 0.171 lr 8.9695e-06 
12/03/2021 09:46:02 - INFO - volta.train_utils -   [GQA]: iter 56916 Ep: 3.86 loss 0.097 score 0.167 lr 8.96649e-06 
12/03/2021 09:46:33 - INFO - volta.train_utils -   [GQA]: iter 56996 Ep: 3.87 loss 0.095 score 0.170 lr 8.96347e-06 
12/03/2021 09:47:04 - INFO - volta.train_utils -   [GQA]: iter 57076 Ep: 3.87 loss 0.091 score 0.170 lr 8.96045e-06 
12/03/2021 09:47:36 - INFO - volta.train_utils -   [GQA]: iter 57156 Ep: 3.88 loss 0.096 score 0.170 lr 8.95744e-06 
12/03/2021 09:48:07 - INFO - volta.train_utils -   [GQA]: iter 57236 Ep: 3.88 loss 0.101 score 0.167 lr 8.95442e-06 
12/03/2021 09:48:38 - INFO - volta.train_utils -   [GQA]: iter 57316 Ep: 3.89 loss 0.092 score 0.172 lr 8.9514e-06 
12/03/2021 09:49:09 - INFO - volta.train_utils -   [GQA]: iter 57396 Ep: 3.90 loss 0.100 score 0.168 lr 8.94839e-06 
12/03/2021 09:49:41 - INFO - volta.train_utils -   [GQA]: iter 57476 Ep: 3.90 loss 0.087 score 0.178 lr 8.94537e-06 
12/03/2021 09:50:12 - INFO - volta.train_utils -   [GQA]: iter 57556 Ep: 3.91 loss 0.088 score 0.174 lr 8.94236e-06 
12/03/2021 09:50:43 - INFO - volta.train_utils -   [GQA]: iter 57636 Ep: 3.91 loss 0.094 score 0.173 lr 8.93934e-06 
12/03/2021 09:51:15 - INFO - volta.train_utils -   [GQA]: iter 57716 Ep: 3.92 loss 0.099 score 0.171 lr 8.93632e-06 
12/03/2021 09:51:46 - INFO - volta.train_utils -   [GQA]: iter 57796 Ep: 3.92 loss 0.101 score 0.166 lr 8.93331e-06 
12/03/2021 09:52:17 - INFO - volta.train_utils -   [GQA]: iter 57876 Ep: 3.93 loss 0.090 score 0.168 lr 8.93029e-06 
12/03/2021 09:52:48 - INFO - volta.train_utils -   [GQA]: iter 57956 Ep: 3.93 loss 0.093 score 0.175 lr 8.92727e-06 
12/03/2021 09:53:19 - INFO - volta.train_utils -   [GQA]: iter 58036 Ep: 3.94 loss 0.095 score 0.172 lr 8.92426e-06 
12/03/2021 09:53:51 - INFO - volta.train_utils -   [GQA]: iter 58116 Ep: 3.94 loss 0.094 score 0.169 lr 8.92124e-06 
12/03/2021 09:54:22 - INFO - volta.train_utils -   [GQA]: iter 58196 Ep: 3.95 loss 0.088 score 0.173 lr 8.91822e-06 
12/03/2021 09:54:54 - INFO - volta.train_utils -   [GQA]: iter 58276 Ep: 3.96 loss 0.094 score 0.171 lr 8.91521e-06 
12/03/2021 09:55:25 - INFO - volta.train_utils -   [GQA]: iter 58356 Ep: 3.96 loss 0.098 score 0.171 lr 8.91219e-06 
12/03/2021 09:55:56 - INFO - volta.train_utils -   [GQA]: iter 58436 Ep: 3.97 loss 0.092 score 0.172 lr 8.90917e-06 
12/03/2021 09:56:28 - INFO - volta.train_utils -   [GQA]: iter 58516 Ep: 3.97 loss 0.091 score 0.171 lr 8.90616e-06 
12/03/2021 09:56:59 - INFO - volta.train_utils -   [GQA]: iter 58596 Ep: 3.98 loss 0.094 score 0.170 lr 8.90314e-06 
12/03/2021 09:57:31 - INFO - volta.train_utils -   [GQA]: iter 58676 Ep: 3.98 loss 0.091 score 0.172 lr 8.90013e-06 
12/03/2021 09:58:02 - INFO - volta.train_utils -   [GQA]: iter 58756 Ep: 3.99 loss 0.102 score 0.168 lr 8.89711e-06 
12/03/2021 09:58:34 - INFO - volta.train_utils -   [GQA]: iter 58836 Ep: 3.99 loss 0.090 score 0.175 lr 8.89409e-06 
12/03/2021 09:59:05 - INFO - volta.train_utils -   [GQA]: iter 58916 Ep: 4.00 loss 0.097 score 0.169 lr 8.89108e-06 
12/03/2021 10:04:09 - INFO - volta.train_utils -   Eval task TASK15 on iteration 58928 
12/03/2021 10:04:09 - INFO - volta.train_utils -   Validation [GQA]: loss 1.907 score 61.130 
12/03/2021 10:04:09 - INFO - __main__ -   ** ** * Saving model * ** ** 
Epoch:  80%|████████  | 4/5 [6:57:44<1:44:16, 6256.10s/it]12/03/2021 10:05:01 - INFO - volta.train_utils -   [GQA]: iter 59008 Ep: 4.00 loss 0.093 score 0.177 lr 8.88783e-06 
12/03/2021 10:05:33 - INFO - volta.train_utils -   [GQA]: iter 59088 Ep: 4.01 loss 0.094 score 0.174 lr 8.88459e-06 
12/03/2021 10:06:04 - INFO - volta.train_utils -   [GQA]: iter 59168 Ep: 4.02 loss 0.090 score 0.170 lr 8.88157e-06 
12/03/2021 10:06:35 - INFO - volta.train_utils -   [GQA]: iter 59248 Ep: 4.02 loss 0.093 score 0.171 lr 8.87856e-06 
12/03/2021 10:07:07 - INFO - volta.train_utils -   [GQA]: iter 59328 Ep: 4.03 loss 0.089 score 0.174 lr 8.87554e-06 
12/03/2021 10:07:38 - INFO - volta.train_utils -   [GQA]: iter 59408 Ep: 4.03 loss 0.092 score 0.173 lr 8.87252e-06 
12/03/2021 10:08:10 - INFO - volta.train_utils -   [GQA]: iter 59488 Ep: 4.04 loss 0.091 score 0.173 lr 8.86951e-06 
12/03/2021 10:08:41 - INFO - volta.train_utils -   [GQA]: iter 59568 Ep: 4.04 loss 0.090 score 0.173 lr 8.86649e-06 
12/03/2021 10:09:12 - INFO - volta.train_utils -   [GQA]: iter 59648 Ep: 4.05 loss 0.094 score 0.174 lr 8.86348e-06 
12/03/2021 10:09:43 - INFO - volta.train_utils -   [GQA]: iter 59728 Ep: 4.05 loss 0.095 score 0.169 lr 8.86046e-06 
12/03/2021 10:10:15 - INFO - volta.train_utils -   [GQA]: iter 59808 Ep: 4.06 loss 0.090 score 0.176 lr 8.85744e-06 
12/03/2021 10:10:46 - INFO - volta.train_utils -   [GQA]: iter 59888 Ep: 4.06 loss 0.093 score 0.172 lr 8.85443e-06 
12/03/2021 10:11:18 - INFO - volta.train_utils -   [GQA]: iter 59968 Ep: 4.07 loss 0.090 score 0.174 lr 8.85141e-06 
12/03/2021 10:11:49 - INFO - volta.train_utils -   [GQA]: iter 60048 Ep: 4.08 loss 0.092 score 0.172 lr 8.84839e-06 
12/03/2021 10:12:20 - INFO - volta.train_utils -   [GQA]: iter 60128 Ep: 4.08 loss 0.092 score 0.171 lr 8.84538e-06 
12/03/2021 10:12:52 - INFO - volta.train_utils -   [GQA]: iter 60208 Ep: 4.09 loss 0.090 score 0.172 lr 8.84236e-06 
12/03/2021 10:13:23 - INFO - volta.train_utils -   [GQA]: iter 60288 Ep: 4.09 loss 0.093 score 0.175 lr 8.83934e-06 
12/03/2021 10:13:54 - INFO - volta.train_utils -   [GQA]: iter 60368 Ep: 4.10 loss 0.090 score 0.177 lr 8.83633e-06 
12/03/2021 10:14:26 - INFO - volta.train_utils -   [GQA]: iter 60448 Ep: 4.10 loss 0.086 score 0.179 lr 8.83331e-06 
12/03/2021 10:14:57 - INFO - volta.train_utils -   [GQA]: iter 60528 Ep: 4.11 loss 0.090 score 0.176 lr 8.83029e-06 
12/03/2021 10:15:28 - INFO - volta.train_utils -   [GQA]: iter 60608 Ep: 4.11 loss 0.097 score 0.174 lr 8.82728e-06 
12/03/2021 10:16:00 - INFO - volta.train_utils -   [GQA]: iter 60688 Ep: 4.12 loss 0.096 score 0.172 lr 8.82426e-06 
12/03/2021 10:16:31 - INFO - volta.train_utils -   [GQA]: iter 60768 Ep: 4.12 loss 0.089 score 0.174 lr 8.82124e-06 
12/03/2021 10:17:03 - INFO - volta.train_utils -   [GQA]: iter 60848 Ep: 4.13 loss 0.085 score 0.176 lr 8.81823e-06 
12/03/2021 10:17:34 - INFO - volta.train_utils -   [GQA]: iter 60928 Ep: 4.14 loss 0.084 score 0.179 lr 8.81521e-06 
12/03/2021 10:18:05 - INFO - volta.train_utils -   [GQA]: iter 61008 Ep: 4.14 loss 0.093 score 0.173 lr 8.8122e-06 
12/03/2021 10:18:36 - INFO - volta.train_utils -   [GQA]: iter 61088 Ep: 4.15 loss 0.093 score 0.171 lr 8.80918e-06 
12/03/2021 10:19:08 - INFO - volta.train_utils -   [GQA]: iter 61168 Ep: 4.15 loss 0.088 score 0.175 lr 8.80616e-06 
12/03/2021 10:19:39 - INFO - volta.train_utils -   [GQA]: iter 61248 Ep: 4.16 loss 0.090 score 0.171 lr 8.80315e-06 
12/03/2021 10:20:11 - INFO - volta.train_utils -   [GQA]: iter 61328 Ep: 4.16 loss 0.092 score 0.173 lr 8.80013e-06 
12/03/2021 10:20:42 - INFO - volta.train_utils -   [GQA]: iter 61408 Ep: 4.17 loss 0.089 score 0.175 lr 8.79711e-06 
12/03/2021 10:21:13 - INFO - volta.train_utils -   [GQA]: iter 61488 Ep: 4.17 loss 0.088 score 0.176 lr 8.7941e-06 
12/03/2021 10:21:45 - INFO - volta.train_utils -   [GQA]: iter 61568 Ep: 4.18 loss 0.091 score 0.174 lr 8.79108e-06 
12/03/2021 10:22:16 - INFO - volta.train_utils -   [GQA]: iter 61648 Ep: 4.18 loss 0.083 score 0.177 lr 8.78806e-06 
12/03/2021 10:22:48 - INFO - volta.train_utils -   [GQA]: iter 61728 Ep: 4.19 loss 0.089 score 0.177 lr 8.78505e-06 
12/03/2021 10:23:19 - INFO - volta.train_utils -   [GQA]: iter 61808 Ep: 4.19 loss 0.094 score 0.172 lr 8.78203e-06 
12/03/2021 10:23:50 - INFO - volta.train_utils -   [GQA]: iter 61888 Ep: 4.20 loss 0.099 score 0.176 lr 8.77901e-06 
12/03/2021 10:24:22 - INFO - volta.train_utils -   [GQA]: iter 61968 Ep: 4.21 loss 0.090 score 0.176 lr 8.776e-06 
12/03/2021 10:24:53 - INFO - volta.train_utils -   [GQA]: iter 62048 Ep: 4.21 loss 0.085 score 0.178 lr 8.77298e-06 
12/03/2021 10:25:25 - INFO - volta.train_utils -   [GQA]: iter 62128 Ep: 4.22 loss 0.090 score 0.172 lr 8.76997e-06 
12/03/2021 10:25:56 - INFO - volta.train_utils -   [GQA]: iter 62208 Ep: 4.22 loss 0.083 score 0.178 lr 8.76695e-06 
12/03/2021 10:26:27 - INFO - volta.train_utils -   [GQA]: iter 62288 Ep: 4.23 loss 0.085 score 0.180 lr 8.76393e-06 
12/03/2021 10:26:59 - INFO - volta.train_utils -   [GQA]: iter 62368 Ep: 4.23 loss 0.093 score 0.173 lr 8.76092e-06 
12/03/2021 10:27:30 - INFO - volta.train_utils -   [GQA]: iter 62448 Ep: 4.24 loss 0.093 score 0.171 lr 8.7579e-06 
12/03/2021 10:28:01 - INFO - volta.train_utils -   [GQA]: iter 62528 Ep: 4.24 loss 0.086 score 0.179 lr 8.75488e-06 
12/03/2021 10:28:33 - INFO - volta.train_utils -   [GQA]: iter 62608 Ep: 4.25 loss 0.091 score 0.177 lr 8.75187e-06 
12/03/2021 10:29:04 - INFO - volta.train_utils -   [GQA]: iter 62688 Ep: 4.25 loss 0.092 score 0.176 lr 8.74885e-06 
12/03/2021 10:29:36 - INFO - volta.train_utils -   [GQA]: iter 62768 Ep: 4.26 loss 0.093 score 0.175 lr 8.74583e-06 
12/03/2021 10:30:07 - INFO - volta.train_utils -   [GQA]: iter 62848 Ep: 4.27 loss 0.086 score 0.176 lr 8.74282e-06 
12/03/2021 10:30:39 - INFO - volta.train_utils -   [GQA]: iter 62928 Ep: 4.27 loss 0.093 score 0.175 lr 8.7398e-06 
12/03/2021 10:31:10 - INFO - volta.train_utils -   [GQA]: iter 63008 Ep: 4.28 loss 0.084 score 0.176 lr 8.73678e-06 
12/03/2021 10:31:41 - INFO - volta.train_utils -   [GQA]: iter 63088 Ep: 4.28 loss 0.089 score 0.176 lr 8.73377e-06 
12/03/2021 10:32:12 - INFO - volta.train_utils -   [GQA]: iter 63168 Ep: 4.29 loss 0.088 score 0.175 lr 8.73075e-06 
12/03/2021 10:32:43 - INFO - volta.train_utils -   [GQA]: iter 63248 Ep: 4.29 loss 0.088 score 0.175 lr 8.72773e-06 
12/03/2021 10:33:14 - INFO - volta.train_utils -   [GQA]: iter 63328 Ep: 4.30 loss 0.090 score 0.175 lr 8.72472e-06 
12/03/2021 10:33:46 - INFO - volta.train_utils -   [GQA]: iter 63408 Ep: 4.30 loss 0.087 score 0.178 lr 8.7217e-06 
12/03/2021 10:34:17 - INFO - volta.train_utils -   [GQA]: iter 63488 Ep: 4.31 loss 0.086 score 0.183 lr 8.71869e-06 
12/03/2021 10:34:48 - INFO - volta.train_utils -   [GQA]: iter 63568 Ep: 4.31 loss 0.084 score 0.178 lr 8.71567e-06 
12/03/2021 10:35:19 - INFO - volta.train_utils -   [GQA]: iter 63648 Ep: 4.32 loss 0.087 score 0.178 lr 8.71265e-06 
12/03/2021 10:35:51 - INFO - volta.train_utils -   [GQA]: iter 63728 Ep: 4.33 loss 0.093 score 0.174 lr 8.70964e-06 
12/03/2021 10:36:22 - INFO - volta.train_utils -   [GQA]: iter 63808 Ep: 4.33 loss 0.090 score 0.173 lr 8.70662e-06 
12/03/2021 10:36:53 - INFO - volta.train_utils -   [GQA]: iter 63888 Ep: 4.34 loss 0.089 score 0.176 lr 8.7036e-06 
12/03/2021 10:37:24 - INFO - volta.train_utils -   [GQA]: iter 63968 Ep: 4.34 loss 0.086 score 0.176 lr 8.70059e-06 
12/03/2021 10:37:55 - INFO - volta.train_utils -   [GQA]: iter 64048 Ep: 4.35 loss 0.092 score 0.178 lr 8.69757e-06 
12/03/2021 10:38:27 - INFO - volta.train_utils -   [GQA]: iter 64128 Ep: 4.35 loss 0.093 score 0.178 lr 8.69455e-06 
12/03/2021 10:38:58 - INFO - volta.train_utils -   [GQA]: iter 64208 Ep: 4.36 loss 0.084 score 0.181 lr 8.69154e-06 
12/03/2021 10:39:29 - INFO - volta.train_utils -   [GQA]: iter 64288 Ep: 4.36 loss 0.094 score 0.176 lr 8.68852e-06 
12/03/2021 10:40:00 - INFO - volta.train_utils -   [GQA]: iter 64368 Ep: 4.37 loss 0.087 score 0.177 lr 8.6855e-06 
12/03/2021 10:40:32 - INFO - volta.train_utils -   [GQA]: iter 64448 Ep: 4.37 loss 0.094 score 0.172 lr 8.68249e-06 
12/03/2021 10:41:03 - INFO - volta.train_utils -   [GQA]: iter 64528 Ep: 4.38 loss 0.093 score 0.177 lr 8.67947e-06 
12/03/2021 10:41:34 - INFO - volta.train_utils -   [GQA]: iter 64608 Ep: 4.38 loss 0.085 score 0.176 lr 8.67646e-06 
12/03/2021 10:42:05 - INFO - volta.train_utils -   [GQA]: iter 64688 Ep: 4.39 loss 0.085 score 0.177 lr 8.67344e-06 
12/03/2021 10:42:37 - INFO - volta.train_utils -   [GQA]: iter 64768 Ep: 4.40 loss 0.085 score 0.178 lr 8.67042e-06 
12/03/2021 10:43:08 - INFO - volta.train_utils -   [GQA]: iter 64848 Ep: 4.40 loss 0.083 score 0.181 lr 8.66741e-06 
12/03/2021 10:43:39 - INFO - volta.train_utils -   [GQA]: iter 64928 Ep: 4.41 loss 0.087 score 0.177 lr 8.66439e-06 
12/03/2021 10:44:10 - INFO - volta.train_utils -   [GQA]: iter 65008 Ep: 4.41 loss 0.086 score 0.180 lr 8.66137e-06 
12/03/2021 10:44:41 - INFO - volta.train_utils -   [GQA]: iter 65088 Ep: 4.42 loss 0.090 score 0.179 lr 8.65836e-06 
12/03/2021 10:45:12 - INFO - volta.train_utils -   [GQA]: iter 65168 Ep: 4.42 loss 0.088 score 0.178 lr 8.65534e-06 
12/03/2021 10:45:44 - INFO - volta.train_utils -   [GQA]: iter 65248 Ep: 4.43 loss 0.077 score 0.181 lr 8.65232e-06 
12/03/2021 10:46:15 - INFO - volta.train_utils -   [GQA]: iter 65328 Ep: 4.43 loss 0.084 score 0.177 lr 8.64931e-06 
12/03/2021 10:46:46 - INFO - volta.train_utils -   [GQA]: iter 65408 Ep: 4.44 loss 0.088 score 0.177 lr 8.64629e-06 
12/03/2021 10:47:18 - INFO - volta.train_utils -   [GQA]: iter 65488 Ep: 4.44 loss 0.079 score 0.181 lr 8.64327e-06 
12/03/2021 10:47:49 - INFO - volta.train_utils -   [GQA]: iter 65568 Ep: 4.45 loss 0.088 score 0.178 lr 8.64026e-06 
12/03/2021 10:48:20 - INFO - volta.train_utils -   [GQA]: iter 65648 Ep: 4.46 loss 0.083 score 0.180 lr 8.63724e-06 
12/03/2021 10:48:51 - INFO - volta.train_utils -   [GQA]: iter 65728 Ep: 4.46 loss 0.090 score 0.178 lr 8.63422e-06 
12/03/2021 10:49:23 - INFO - volta.train_utils -   [GQA]: iter 65808 Ep: 4.47 loss 0.086 score 0.178 lr 8.63121e-06 
12/03/2021 10:49:54 - INFO - volta.train_utils -   [GQA]: iter 65888 Ep: 4.47 loss 0.091 score 0.180 lr 8.62819e-06 
12/03/2021 10:50:25 - INFO - volta.train_utils -   [GQA]: iter 65968 Ep: 4.48 loss 0.091 score 0.176 lr 8.62518e-06 
12/03/2021 10:50:56 - INFO - volta.train_utils -   [GQA]: iter 66048 Ep: 4.48 loss 0.089 score 0.179 lr 8.62216e-06 
12/03/2021 10:51:28 - INFO - volta.train_utils -   [GQA]: iter 66128 Ep: 4.49 loss 0.089 score 0.183 lr 8.61914e-06 
12/03/2021 10:51:59 - INFO - volta.train_utils -   [GQA]: iter 66208 Ep: 4.49 loss 0.088 score 0.178 lr 8.61613e-06 
12/03/2021 10:52:30 - INFO - volta.train_utils -   [GQA]: iter 66288 Ep: 4.50 loss 0.082 score 0.179 lr 8.61311e-06 
12/03/2021 10:53:01 - INFO - volta.train_utils -   [GQA]: iter 66368 Ep: 4.50 loss 0.085 score 0.180 lr 8.61009e-06 
12/03/2021 10:53:32 - INFO - volta.train_utils -   [GQA]: iter 66448 Ep: 4.51 loss 0.080 score 0.180 lr 8.60708e-06 
12/03/2021 10:54:04 - INFO - volta.train_utils -   [GQA]: iter 66528 Ep: 4.52 loss 0.086 score 0.175 lr 8.60406e-06 
12/03/2021 10:54:35 - INFO - volta.train_utils -   [GQA]: iter 66608 Ep: 4.52 loss 0.095 score 0.177 lr 8.60104e-06 
12/03/2021 10:55:06 - INFO - volta.train_utils -   [GQA]: iter 66688 Ep: 4.53 loss 0.087 score 0.179 lr 8.59803e-06 
12/03/2021 10:55:37 - INFO - volta.train_utils -   [GQA]: iter 66768 Ep: 4.53 loss 0.081 score 0.185 lr 8.59501e-06 
12/03/2021 10:56:08 - INFO - volta.train_utils -   [GQA]: iter 66848 Ep: 4.54 loss 0.083 score 0.180 lr 8.59199e-06 
12/03/2021 10:56:39 - INFO - volta.train_utils -   [GQA]: iter 66928 Ep: 4.54 loss 0.084 score 0.181 lr 8.58898e-06 
12/03/2021 10:57:11 - INFO - volta.train_utils -   [GQA]: iter 67008 Ep: 4.55 loss 0.086 score 0.179 lr 8.58596e-06 
12/03/2021 10:57:42 - INFO - volta.train_utils -   [GQA]: iter 67088 Ep: 4.55 loss 0.084 score 0.177 lr 8.58294e-06 
12/03/2021 10:58:14 - INFO - volta.train_utils -   [GQA]: iter 67168 Ep: 4.56 loss 0.083 score 0.184 lr 8.57993e-06 
12/03/2021 10:58:45 - INFO - volta.train_utils -   [GQA]: iter 67248 Ep: 4.56 loss 0.088 score 0.180 lr 8.57691e-06 
12/03/2021 10:59:16 - INFO - volta.train_utils -   [GQA]: iter 67328 Ep: 4.57 loss 0.083 score 0.179 lr 8.5739e-06 
12/03/2021 10:59:48 - INFO - volta.train_utils -   [GQA]: iter 67408 Ep: 4.57 loss 0.085 score 0.181 lr 8.57088e-06 
12/03/2021 11:00:19 - INFO - volta.train_utils -   [GQA]: iter 67488 Ep: 4.58 loss 0.081 score 0.180 lr 8.56786e-06 
12/03/2021 11:00:51 - INFO - volta.train_utils -   [GQA]: iter 67568 Ep: 4.59 loss 0.082 score 0.182 lr 8.56485e-06 
12/03/2021 11:01:22 - INFO - volta.train_utils -   [GQA]: iter 67648 Ep: 4.59 loss 0.079 score 0.180 lr 8.56183e-06 
12/03/2021 11:01:54 - INFO - volta.train_utils -   [GQA]: iter 67728 Ep: 4.60 loss 0.078 score 0.183 lr 8.55881e-06 
12/03/2021 11:02:25 - INFO - volta.train_utils -   [GQA]: iter 67808 Ep: 4.60 loss 0.088 score 0.179 lr 8.5558e-06 
12/03/2021 11:02:57 - INFO - volta.train_utils -   [GQA]: iter 67888 Ep: 4.61 loss 0.088 score 0.179 lr 8.55278e-06 
12/03/2021 11:03:29 - INFO - volta.train_utils -   [GQA]: iter 67968 Ep: 4.61 loss 0.083 score 0.180 lr 8.54976e-06 
12/03/2021 11:04:00 - INFO - volta.train_utils -   [GQA]: iter 68048 Ep: 4.62 loss 0.086 score 0.176 lr 8.54675e-06 
12/03/2021 11:04:31 - INFO - volta.train_utils -   [GQA]: iter 68128 Ep: 4.62 loss 0.082 score 0.182 lr 8.54373e-06 
12/03/2021 11:05:02 - INFO - volta.train_utils -   [GQA]: iter 68208 Ep: 4.63 loss 0.086 score 0.177 lr 8.54071e-06 
12/03/2021 11:05:33 - INFO - volta.train_utils -   [GQA]: iter 68288 Ep: 4.63 loss 0.086 score 0.179 lr 8.5377e-06 
12/03/2021 11:06:04 - INFO - volta.train_utils -   [GQA]: iter 68368 Ep: 4.64 loss 0.078 score 0.182 lr 8.53468e-06 
12/03/2021 11:06:36 - INFO - volta.train_utils -   [GQA]: iter 68448 Ep: 4.65 loss 0.082 score 0.186 lr 8.53167e-06 
12/03/2021 11:07:07 - INFO - volta.train_utils -   [GQA]: iter 68528 Ep: 4.65 loss 0.086 score 0.179 lr 8.52865e-06 
12/03/2021 11:07:38 - INFO - volta.train_utils -   [GQA]: iter 68608 Ep: 4.66 loss 0.084 score 0.181 lr 8.52563e-06 
12/03/2021 11:08:09 - INFO - volta.train_utils -   [GQA]: iter 68688 Ep: 4.66 loss 0.086 score 0.181 lr 8.52262e-06 
12/03/2021 11:08:40 - INFO - volta.train_utils -   [GQA]: iter 68768 Ep: 4.67 loss 0.085 score 0.181 lr 8.5196e-06 
12/03/2021 11:09:12 - INFO - volta.train_utils -   [GQA]: iter 68848 Ep: 4.67 loss 0.081 score 0.181 lr 8.51658e-06 
12/03/2021 11:09:43 - INFO - volta.train_utils -   [GQA]: iter 68928 Ep: 4.68 loss 0.084 score 0.180 lr 8.51357e-06 
12/03/2021 11:10:14 - INFO - volta.train_utils -   [GQA]: iter 69008 Ep: 4.68 loss 0.081 score 0.177 lr 8.51055e-06 
12/03/2021 11:10:45 - INFO - volta.train_utils -   [GQA]: iter 69088 Ep: 4.69 loss 0.083 score 0.182 lr 8.50753e-06 
12/03/2021 11:11:17 - INFO - volta.train_utils -   [GQA]: iter 69168 Ep: 4.69 loss 0.083 score 0.182 lr 8.50452e-06 
12/03/2021 11:11:48 - INFO - volta.train_utils -   [GQA]: iter 69248 Ep: 4.70 loss 0.086 score 0.180 lr 8.5015e-06 
12/03/2021 11:12:19 - INFO - volta.train_utils -   [GQA]: iter 69328 Ep: 4.71 loss 0.089 score 0.180 lr 8.49848e-06 
12/03/2021 11:12:50 - INFO - volta.train_utils -   [GQA]: iter 69408 Ep: 4.71 loss 0.079 score 0.183 lr 8.49547e-06 
12/03/2021 11:13:21 - INFO - volta.train_utils -   [GQA]: iter 69488 Ep: 4.72 loss 0.080 score 0.185 lr 8.49245e-06 
12/03/2021 11:13:52 - INFO - volta.train_utils -   [GQA]: iter 69568 Ep: 4.72 loss 0.081 score 0.185 lr 8.48943e-06 
12/03/2021 11:14:24 - INFO - volta.train_utils -   [GQA]: iter 69648 Ep: 4.73 loss 0.086 score 0.181 lr 8.48642e-06 
12/03/2021 11:14:55 - INFO - volta.train_utils -   [GQA]: iter 69728 Ep: 4.73 loss 0.080 score 0.187 lr 8.4834e-06 
12/03/2021 11:15:26 - INFO - volta.train_utils -   [GQA]: iter 69808 Ep: 4.74 loss 0.080 score 0.184 lr 8.48039e-06 
12/03/2021 11:15:57 - INFO - volta.train_utils -   [GQA]: iter 69888 Ep: 4.74 loss 0.079 score 0.187 lr 8.47737e-06 
12/03/2021 11:16:28 - INFO - volta.train_utils -   [GQA]: iter 69968 Ep: 4.75 loss 0.079 score 0.185 lr 8.47435e-06 
12/03/2021 11:17:00 - INFO - volta.train_utils -   [GQA]: iter 70048 Ep: 4.75 loss 0.079 score 0.182 lr 8.47134e-06 
12/03/2021 11:17:31 - INFO - volta.train_utils -   [GQA]: iter 70128 Ep: 4.76 loss 0.079 score 0.184 lr 8.46832e-06 
12/03/2021 11:18:02 - INFO - volta.train_utils -   [GQA]: iter 70208 Ep: 4.77 loss 0.077 score 0.182 lr 8.4653e-06 
12/03/2021 11:18:33 - INFO - volta.train_utils -   [GQA]: iter 70288 Ep: 4.77 loss 0.083 score 0.182 lr 8.46229e-06 
12/03/2021 11:19:04 - INFO - volta.train_utils -   [GQA]: iter 70368 Ep: 4.78 loss 0.075 score 0.185 lr 8.45927e-06 
12/03/2021 11:19:36 - INFO - volta.train_utils -   [GQA]: iter 70448 Ep: 4.78 loss 0.079 score 0.186 lr 8.45625e-06 
12/03/2021 11:20:07 - INFO - volta.train_utils -   [GQA]: iter 70528 Ep: 4.79 loss 0.074 score 0.184 lr 8.45324e-06 
12/03/2021 11:20:38 - INFO - volta.train_utils -   [GQA]: iter 70608 Ep: 4.79 loss 0.083 score 0.183 lr 8.45022e-06 
12/03/2021 11:21:09 - INFO - volta.train_utils -   [GQA]: iter 70688 Ep: 4.80 loss 0.087 score 0.182 lr 8.4472e-06 
12/03/2021 11:21:41 - INFO - volta.train_utils -   [GQA]: iter 70768 Ep: 4.80 loss 0.076 score 0.187 lr 8.44419e-06 
12/03/2021 11:22:12 - INFO - volta.train_utils -   [GQA]: iter 70848 Ep: 4.81 loss 0.080 score 0.184 lr 8.44117e-06 
12/03/2021 11:22:43 - INFO - volta.train_utils -   [GQA]: iter 70928 Ep: 4.81 loss 0.084 score 0.180 lr 8.43816e-06 
12/03/2021 11:23:14 - INFO - volta.train_utils -   [GQA]: iter 71008 Ep: 4.82 loss 0.082 score 0.182 lr 8.43514e-06 
12/03/2021 11:23:46 - INFO - volta.train_utils -   [GQA]: iter 71088 Ep: 4.82 loss 0.086 score 0.184 lr 8.43212e-06 
12/03/2021 11:24:17 - INFO - volta.train_utils -   [GQA]: iter 71168 Ep: 4.83 loss 0.080 score 0.182 lr 8.42911e-06 
12/03/2021 11:24:48 - INFO - volta.train_utils -   [GQA]: iter 71248 Ep: 4.84 loss 0.082 score 0.186 lr 8.42609e-06 
12/03/2021 11:25:19 - INFO - volta.train_utils -   [GQA]: iter 71328 Ep: 4.84 loss 0.080 score 0.181 lr 8.42307e-06 
12/03/2021 11:25:50 - INFO - volta.train_utils -   [GQA]: iter 71408 Ep: 4.85 loss 0.088 score 0.184 lr 8.42006e-06 
12/03/2021 11:26:22 - INFO - volta.train_utils -   [GQA]: iter 71488 Ep: 4.85 loss 0.074 score 0.189 lr 8.41704e-06 
12/03/2021 11:26:53 - INFO - volta.train_utils -   [GQA]: iter 71568 Ep: 4.86 loss 0.085 score 0.182 lr 8.41402e-06 
12/03/2021 11:27:24 - INFO - volta.train_utils -   [GQA]: iter 71648 Ep: 4.86 loss 0.085 score 0.181 lr 8.41101e-06 
12/03/2021 11:27:55 - INFO - volta.train_utils -   [GQA]: iter 71728 Ep: 4.87 loss 0.078 score 0.183 lr 8.40799e-06 
12/03/2021 11:28:27 - INFO - volta.train_utils -   [GQA]: iter 71808 Ep: 4.87 loss 0.085 score 0.183 lr 8.40497e-06 
12/03/2021 11:28:58 - INFO - volta.train_utils -   [GQA]: iter 71888 Ep: 4.88 loss 0.081 score 0.185 lr 8.40196e-06 
12/03/2021 11:29:29 - INFO - volta.train_utils -   [GQA]: iter 71968 Ep: 4.88 loss 0.078 score 0.182 lr 8.39894e-06 
12/03/2021 11:30:01 - INFO - volta.train_utils -   [GQA]: iter 72048 Ep: 4.89 loss 0.076 score 0.188 lr 8.39592e-06 
12/03/2021 11:30:32 - INFO - volta.train_utils -   [GQA]: iter 72128 Ep: 4.90 loss 0.085 score 0.185 lr 8.39291e-06 
12/03/2021 11:31:03 - INFO - volta.train_utils -   [GQA]: iter 72208 Ep: 4.90 loss 0.074 score 0.190 lr 8.38989e-06 
12/03/2021 11:31:34 - INFO - volta.train_utils -   [GQA]: iter 72288 Ep: 4.91 loss 0.072 score 0.188 lr 8.38688e-06 
12/03/2021 11:32:06 - INFO - volta.train_utils -   [GQA]: iter 72368 Ep: 4.91 loss 0.077 score 0.185 lr 8.38386e-06 
12/03/2021 11:32:37 - INFO - volta.train_utils -   [GQA]: iter 72448 Ep: 4.92 loss 0.078 score 0.185 lr 8.38084e-06 
12/03/2021 11:33:08 - INFO - volta.train_utils -   [GQA]: iter 72528 Ep: 4.92 loss 0.081 score 0.183 lr 8.37783e-06 
12/03/2021 11:33:40 - INFO - volta.train_utils -   [GQA]: iter 72608 Ep: 4.93 loss 0.081 score 0.181 lr 8.37481e-06 
12/03/2021 11:34:11 - INFO - volta.train_utils -   [GQA]: iter 72688 Ep: 4.93 loss 0.076 score 0.188 lr 8.37179e-06 
12/03/2021 11:34:43 - INFO - volta.train_utils -   [GQA]: iter 72768 Ep: 4.94 loss 0.076 score 0.185 lr 8.36878e-06 
12/03/2021 11:35:14 - INFO - volta.train_utils -   [GQA]: iter 72848 Ep: 4.94 loss 0.082 score 0.182 lr 8.36576e-06 
12/03/2021 11:35:45 - INFO - volta.train_utils -   [GQA]: iter 72928 Ep: 4.95 loss 0.078 score 0.186 lr 8.36274e-06 
12/03/2021 11:36:17 - INFO - volta.train_utils -   [GQA]: iter 73008 Ep: 4.96 loss 0.080 score 0.185 lr 8.35973e-06 
12/03/2021 11:36:48 - INFO - volta.train_utils -   [GQA]: iter 73088 Ep: 4.96 loss 0.077 score 0.186 lr 8.35671e-06 
12/03/2021 11:37:20 - INFO - volta.train_utils -   [GQA]: iter 73168 Ep: 4.97 loss 0.080 score 0.185 lr 8.35369e-06 
12/03/2021 11:37:51 - INFO - volta.train_utils -   [GQA]: iter 73248 Ep: 4.97 loss 0.080 score 0.186 lr 8.35068e-06 
12/03/2021 11:38:22 - INFO - volta.train_utils -   [GQA]: iter 73328 Ep: 4.98 loss 0.088 score 0.182 lr 8.34766e-06 
12/03/2021 11:38:54 - INFO - volta.train_utils -   [GQA]: iter 73408 Ep: 4.98 loss 0.079 score 0.186 lr 8.34465e-06 
12/03/2021 11:39:25 - INFO - volta.train_utils -   [GQA]: iter 73488 Ep: 4.99 loss 0.083 score 0.179 lr 8.34163e-06 
12/03/2021 11:39:57 - INFO - volta.train_utils -   [GQA]: iter 73568 Ep: 4.99 loss 0.075 score 0.187 lr 8.33861e-06 
12/03/2021 11:40:28 - INFO - volta.train_utils -   [GQA]: iter 73648 Ep: 5.00 loss 0.085 score 0.185 lr 8.3356e-06 
12/03/2021 11:45:35 - INFO - volta.train_utils -   Eval task TASK15 on iteration 73660 
12/03/2021 11:45:35 - INFO - volta.train_utils -   Validation [GQA]: loss 1.950 score 61.968 
12/03/2021 11:45:35 - INFO - __main__ -   ** ** * Saving model * ** ** 
Epoch: 100%|██████████| 5/5 [8:39:09<00:00, 6204.53s/it]  
